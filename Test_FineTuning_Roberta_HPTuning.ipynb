{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U adapter-transformers\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration nsusemiehl--SciERC-f57c64a52b9c80c0\n",
      "Reusing dataset json (C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 3/3 [00:00<00:00, 749.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3219, 'test': 974, 'validation': 455}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "scierc_name = 'nsusemiehl/SciERC'\n",
    "scierc_dataset = load_dataset(scierc_name)\n",
    "print(scierc_dataset.num_rows)\n",
    "\n",
    "# acl_arc_name = 'zapsdcn/citation_intent'\n",
    "# acl_arc_dataset = load_dataset(acl_arc_name)\n",
    "# print(acl_arc_dataset.num_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The agreement in question involves number in [[ nouns ]] and << reflexive pronouns >> and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .',\n",
       " 'label': 'CONJUNCTION',\n",
       " 'metadata': [7, 7, 9, 10]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scierc_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMPARE' 'CONJUNCTION' 'EVALUATE-FOR' 'FEATURE-OF' 'HYPONYM-OF'\n",
      " 'PART-OF' 'USED-FOR']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Finding the number of labels\n",
    "import numpy as np\n",
    "labels = np.unique(np.array(scierc_dataset['train']['label']))\n",
    "num_of_labels = labels.size\n",
    "\n",
    "print(labels)\n",
    "print(num_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-3fa4decd4606a523.arrow\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-4d2e52dbe4cdbad6.arrow\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-214c6724dd02783d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'The agreement in question involves number in [[ nouns ]] and << reflexive pronouns >> and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .',\n",
       " 'label': 1,\n",
       " 'metadata': [7, 7, 9, 10]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding the labels\n",
    "def encode_labels(dataset):\n",
    "    for i in range(num_of_labels):\n",
    "        if dataset['label'] == labels[i]:\n",
    "            dataset['label'] = i\n",
    "    return dataset\n",
    "\n",
    "scierc_dataset = scierc_dataset.map(encode_labels)\n",
    "scierc_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-66a8ac042a6f2d30.arrow\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-10b38373a51fb787.arrow\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-500b8b5d9ccb8f86.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Encode the input data\n",
    "scierc_dataset = scierc_dataset.map(encode_batch, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "scierc_dataset = scierc_dataset.rename_column(\"label\", 'labels')\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "scierc_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaAdapterModel\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    num_labels=num_of_labels,\n",
    ")\n",
    "model = RobertaAdapterModel.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")\n",
    "# Add new adapter\n",
    "model.add_adapter(\"sci_erc\")\n",
    "# Add a matching classification head\n",
    "model.add_classification_head(\n",
    "    \"sci_erc\",\n",
    "    num_labels=num_of_labels,\n",
    "    id2label={0:'COMPARE', 1:'CONJUNCTION', 2:'EVALUATE-FOR', \n",
    "            3:'FEATURE-OF', 4:'HYPONYM-OF', 5:'PART-OF', 6:'USED-FOR'}\n",
    ")\n",
    "# Activate the adapter\n",
    "model.train_adapter(\"sci_erc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to create a function to create a new model for hyperparameter tuning to work. This allows the tuner to create a new model to test on.\n",
    "def model_init():\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "        \"roberta-base\",\n",
    "        num_labels=num_of_labels,\n",
    "    )\n",
    "    model = RobertaAdapterModel.from_pretrained(\n",
    "        \"roberta-base\",\n",
    "        config=config,\n",
    "    )\n",
    "    # Add new adapter\n",
    "    model.add_adapter(\"sci_erc\")\n",
    "    # Add a matching classification head\n",
    "    model.add_classification_head(\n",
    "        \"sci_erc\",\n",
    "        num_labels=num_of_labels,\n",
    "        id2label={0:'COMPARE', 1:'CONJUNCTION', 2:'EVALUATE-FOR', \n",
    "                3:'FEATURE-OF', 4:'HYPONYM-OF', 5:'PART-OF', 6:'USED-FOR'}\n",
    "    )\n",
    "    # Activate the adapter\n",
    "    model.train_adapter(\"sci_erc\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:327: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
      "  warnings.warn(\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "# We want to minimize the loss function so we should not include f1.\n",
    "\n",
    "# Adding Support for Tensorboard, supposedly you don't have to do this, but I find that it doesn't work\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "writer = SummaryWriter()\n",
    "writer = TensorBoardCallback(writer)\n",
    "\n",
    "# This will throw a warning that we should not have both a model and a model_init, but adaptertrainer does not like it when we don't have a model.\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=scierc_dataset[\"train\"],\n",
    "    eval_dataset=scierc_dataset[\"validation\"],\n",
    "    model_init=model_init,\n",
    "    callbacks=[writer]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 200/204 [06:19<00:08,  2.12s/it]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8147, 'learning_rate': 1.0166519921667562e-07, 'epoch': 3.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 98%|█████████▊| 200/204 [06:27<00:08,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8343709707260132, 'eval_runtime': 7.658, 'eval_samples_per_second': 59.415, 'eval_steps_per_second': 0.522, 'epoch': 3.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 204/204 [06:34<00:00,  2.49s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 204/204 [06:34<00:00,  1.93s/it]\n",
      "\u001b[32m[I 2022-04-20 16:26:46,716]\u001b[0m Trial 4 finished with value: 1.8343709707260132 and parameters: {'learning_rate': 5.184925160050457e-06, 'num_train_epochs': 4, 'seed': 27, 'per_device_train_batch_size': 64}. Best is trial 2 with value: 1.3941717147827148.\u001b[0m\n",
      "Trial:\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 394.2735, 'train_samples_per_second': 32.658, 'train_steps_per_second': 0.517, 'train_loss': 1.8578294188368554, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding adapter 'sci_erc'.\n",
      "Adding head 'sci_erc' with config {'head_type': 'classification', 'num_labels': 7, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'COMPARE': 0, 'CONJUNCTION': 1, 'EVALUATE-FOR': 2, 'FEATURE-OF': 3, 'HYPONYM-OF': 4, 'PART-OF': 5, 'USED-FOR': 6}, 'use_pooler': False, 'bias': True}.\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3219\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 303\n",
      " 14%|█▎        | 41/303 [00:44<04:48,  1.10s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\Test_FineTuning_Roberta_HPTuning.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/Test_FineTuning_Roberta_HPTuning.ipynb#ch0000010?line=0'>1</a>\u001b[0m \u001b[39m# We are using optuna for tuning, by default it tries epochs, learning rate, and batch siz, we can try to give it more parameters to search but I feel like these might be the best for now.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/Test_FineTuning_Roberta_HPTuning.ipynb#ch0000010?line=1'>2</a>\u001b[0m \u001b[39m# This will by default run a 100 trials.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/Test_FineTuning_Roberta_HPTuning.ipynb#ch0000010?line=3'>4</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mhyperparameter_search(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/Test_FineTuning_Roberta_HPTuning.ipynb#ch0000010?line=4'>5</a>\u001b[0m     \u001b[39m# hp_space = {Parameters to tune}\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/Test_FineTuning_Roberta_HPTuning.ipynb#ch0000010?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1866\u001b[0m, in \u001b[0;36mTrainer.hyperparameter_search\u001b[1;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1857'>1858</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_objective \u001b[39m=\u001b[39m default_compute_objective \u001b[39mif\u001b[39;00m compute_objective \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m compute_objective\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1859'>1860</a>\u001b[0m backend_dict \u001b[39m=\u001b[39m {\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1860'>1861</a>\u001b[0m     HPSearchBackend\u001b[39m.\u001b[39mOPTUNA: run_hp_search_optuna,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1861'>1862</a>\u001b[0m     HPSearchBackend\u001b[39m.\u001b[39mRAY: run_hp_search_ray,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1862'>1863</a>\u001b[0m     HPSearchBackend\u001b[39m.\u001b[39mSIGOPT: run_hp_search_sigopt,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1863'>1864</a>\u001b[0m     HPSearchBackend\u001b[39m.\u001b[39mWANDB: run_hp_search_wandb,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1864'>1865</a>\u001b[0m }\n\u001b[1;32m-> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1865'>1866</a>\u001b[0m best_run \u001b[39m=\u001b[39m backend_dict[backend](\u001b[39mself\u001b[39m, n_trials, direction, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1867'>1868</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_search_backend \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1868'>1869</a>\u001b[0m \u001b[39mreturn\u001b[39;00m best_run\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\integrations.py:164\u001b[0m, in \u001b[0;36mrun_hp_search_optuna\u001b[1;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=161'>162</a>\u001b[0m n_jobs \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=162'>163</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39mdirection, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=163'>164</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(_objective, n_trials\u001b[39m=\u001b[39;49mn_trials, timeout\u001b[39m=\u001b[39;49mtimeout, n_jobs\u001b[39m=\u001b[39;49mn_jobs)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=164'>165</a>\u001b[0m best_trial \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=165'>166</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BestRun(\u001b[39mstr\u001b[39m(best_trial\u001b[39m.\u001b[39mnumber), best_trial\u001b[39m.\u001b[39mvalue, best_trial\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=391'>392</a>\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=392'>393</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=393'>394</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=394'>395</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=395'>396</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=396'>397</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=397'>398</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=399'>400</a>\u001b[0m _optimize(\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=400'>401</a>\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=401'>402</a>\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=402'>403</a>\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=403'>404</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=404'>405</a>\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=405'>406</a>\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=406'>407</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=407'>408</a>\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=408'>409</a>\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/study.py?line=409'>410</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=63'>64</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=64'>65</a>\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=65'>66</a>\u001b[0m         _optimize_sequential(\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=66'>67</a>\u001b[0m             study,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=67'>68</a>\u001b[0m             func,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=68'>69</a>\u001b[0m             n_trials,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=69'>70</a>\u001b[0m             timeout,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=70'>71</a>\u001b[0m             catch,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=71'>72</a>\u001b[0m             callbacks,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=72'>73</a>\u001b[0m             gc_after_trial,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=73'>74</a>\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=74'>75</a>\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=75'>76</a>\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=76'>77</a>\u001b[0m         )\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=77'>78</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=78'>79</a>\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=159'>160</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=161'>162</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=162'>163</a>\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=163'>164</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=164'>165</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=209'>210</a>\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=211'>212</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=212'>213</a>\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=213'>214</a>\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=214'>215</a>\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/optuna/study/_optimize.py?line=215'>216</a>\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\integrations.py:154\u001b[0m, in \u001b[0;36mrun_hp_search_optuna.<locals>._objective\u001b[1;34m(trial, checkpoint_dir)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=151'>152</a>\u001b[0m             checkpoint \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_dir, subdir)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=152'>153</a>\u001b[0m trainer\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=153'>154</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49mcheckpoint, trial\u001b[39m=\u001b[39;49mtrial)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=154'>155</a>\u001b[0m \u001b[39m# If there hasn't been any evaluation during the training loop.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/integrations.py?line=155'>156</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(trainer, \u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1377\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1373'>1374</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1374'>1375</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1376'>1377</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1377'>1378</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1378'>1379</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1379'>1380</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1380'>1381</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1381'>1382</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1382'>1383</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1383'>1384</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We are using optuna for tuning, by default it tries epochs, learning rate, and batch siz, we can try to give it more parameters to search but I feel like these might be the best for now.\n",
    "# This will by default run a 100 trials.\n",
    "\n",
    "trainer.hyperparameter_search(\n",
    "    # hp_space = {Parameters to tune}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(scierc_dataset['test'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469684d7fc26b35c5046a1b9c559332af356cccfef9c5dca4231e653832987c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
