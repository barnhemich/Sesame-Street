{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "# %conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "# %pip install -U adapter-transformers\n",
    "# %conda install -y -c conda-forge tensorboard\n",
    "# %pip install optuna\n",
    "# %pip install tqdm\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration zapsdcn--citation_intent-0b0f6658161cc990\n",
      "Reusing dataset json (C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\zapsdcn--citation_intent-0b0f6658161cc990\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1498.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 1688, 'test': 139, 'validation': 114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset_name = 'nsusemiehl/SciERC'\n",
    "dataset_name = 'zapsdcn/citation_intent'\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset_name = 'citation_intent'\n",
    "\n",
    "print(dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( Bobrow et al. , 1977 ; Chu-Carroll , 1999 ) .',\n",
       " 'label': 'Future',\n",
       " 'metadata': {}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block creates dataset for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'metadata']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\zapsdcn--citation_intent-0b0f6658161cc990\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-69c4b714d94b638b.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.34ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.14ba/s]\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\zapsdcn--citation_intent-0b0f6658161cc990\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-c2962fc358ac477f.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00, 39.96ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.58ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# Tokenize the set for the transformer\n",
    "def encode_batch_pretraining(batch):\n",
    "    \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Encode the input data\n",
    "# NOTE: num_proc does not seem to work, for some reason it can't find the tokenizer\n",
    "print(dataset['train'].column_names)\n",
    "dataset_pretraining = dataset.map(encode_batch_pretraining, batched=True, remove_columns=dataset['train'].column_names,)\n",
    "\n",
    "# We make the labels the same as the input as this is language learning \n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "  \n",
    "dataset_pretraining = dataset_pretraining.map(add_labels, batched=True)\n",
    "dataset_pretraining.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collater adds padding in the form of EOS tokens, makes data augmentations of random masking ('mlm_probability)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating the dataset for task finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Background' 'CompareOrContrast' 'Extends' 'Future' 'Motivation' 'Uses']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Finding the number of labels\n",
    "import numpy as np\n",
    "labels = np.unique(np.array(dataset['train']['label']))\n",
    "num_of_labels = labels.size\n",
    "\n",
    "print(labels)\n",
    "print(num_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the labels\n",
    "def encode_labels(dataset):\n",
    "    for i in range(num_of_labels):\n",
    "        if dataset['label'] == labels[i]:\n",
    "            dataset['label'] = i\n",
    "    return dataset\n",
    "\n",
    "if dataset_name == 'zapsdcn/citation_intent':\n",
    "    dataset = dataset.map(encode_labels, remove_columns=[\"metadata\"])\n",
    "else:\n",
    "    dataset = dataset.map(encode_labels)\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.82ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.26ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.00ba/s]\n"
     ]
    }
   ],
   "source": [
    "def encode_batch_finetuning(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Encode the input data\n",
    "dataset_finetuning = dataset.map(encode_batch_finetuning, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "dataset_finetuning = dataset_finetuning.rename_column(\"label\", 'labels')\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "dataset_finetuning.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaAdapterModel\n",
    "\n",
    "def model_init(adapter_name = 'default_adapter', \n",
    "               num_lables = 0, \n",
    "               pretraining = False,\n",
    "               load_adapter = False,\n",
    "               adapter_dir = 'path'):\n",
    "    \"\"\"Creates a new roBERTa model with the given name for its adapter.\n",
    "\n",
    "    Args:\n",
    "        adapter_name (str): The name of the adapter to load/create. Defaults to 'default_adapter'.\n",
    "        num_lables (int, optional): The number of labels for classification task. Defaults to 0.\n",
    "        pretraining (bool, optional): Whether to create a model for pretraining or classification. Defaults to False.\n",
    "        load_adapter (bool, optional): Whether to load an adapter with the adapter_name given or create a new one. Defaults to False.\n",
    "        adapter_dir (str, optional): Directory to load the adapter. If load_adapter you need to specify this.  Defaults to 'path'.\n",
    "\n",
    "    Returns:\n",
    "        RobertaAdapterModel: A roBERTA model with an adapter added to it.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pretraining:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            # num_labels=num_of_labels,*-8536.22.03\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        if load_adapter:\n",
    "            # Add new adapter\n",
    "            model.load_adapter(adapter_dir)\n",
    "\n",
    "        else:\n",
    "            # Add new adapter\n",
    "            model.add_adapter(adapter_name)\n",
    "            \n",
    "        # Add a matching classification head\n",
    "        model.add_masked_lm_head(adapter_name)\n",
    "            \n",
    "    else:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            num_labels=num_lables,\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        if load_adapter:\n",
    "            # Add new adapter\n",
    "            model.load_adapter(adapter_dir)\n",
    "\n",
    "        else:\n",
    "            # Add new adapter\n",
    "            model.add_adapter(adapter_name)\n",
    "            \n",
    "        # Add a matching classification head\n",
    "        model.add_classification_head(\n",
    "                adapter_name,\n",
    "                num_labels=num_lables,\n",
    "                id2label={0:'Background', 1:'CompareOrContrast', 2:'Extends', \n",
    "                        3:'Future', 4:'Motivation', 5:'Uses'},\n",
    "                overwrite_ok = True)\n",
    "            \n",
    "    # Activate the adapter\n",
    "    model.train_adapter(adapter_name)    \n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer\n",
    "from datasets import load_metric\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "import json\n",
    "\n",
    "def pretraining_loop(num_models, training_args, dataset, \n",
    "                     data_collator, adapter_name, \n",
    "                    #  DAPT_n_TAPT, TAPT_dataset\n",
    "                     ):\n",
    "    \"\"\"The Loop for running num_models number of models to account for run2run variance. Will run the model \n",
    "        and evaluate.\n",
    "\n",
    "    Args:\n",
    "        num_models (int): Number of models to loop through\n",
    "        training_args (transformers.TrainingArguments): The arguments to pass to the trainer\n",
    "        dataset (dataset): The dataset to train on\n",
    "        data_collator (data_collator): The data collator for the trainer to use\n",
    "        adapter_name (str): Name of the adapter to create\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter, pretraining=True)\n",
    "        \n",
    "        writer = SummaryWriter(log_dir= f'runs/{adapter}')\n",
    "        writer = TensorBoardCallback(writer)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            data_collator=data_collator,  \n",
    "            callbacks=[writer] \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        f = open(f\"{training_args.output_dir}/evaulations.txt\", \"a\")\n",
    "        f.write(adapter)\n",
    "        f.write(json.dumps(trainer.evaluate(dataset['test'])))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        model.save_all_adapters(training_args.output_dir, with_head=False)\n",
    "        # model.save_pretrained(f\"{adapter_name}\")\n",
    "        \n",
    "        # if DAPT_n_TAPT:\n",
    "        #     trainer = AdapterTrainer(\n",
    "        #         model=model,\n",
    "        #         args=training_args,\n",
    "        #         train_dataset=TAPT_dataset[\"train\"],\n",
    "        #         eval_dataset=TAPT_dataset[\"validation\"],\n",
    "        #         data_collator=data_collator,  \n",
    "        #         callbacks=[writer] \n",
    "        #     )\n",
    "            \n",
    "        #     trainer.train()\n",
    "        \n",
    "        #     f = open(\"DAPT_TAPT_evaulations.txt\", \"a\")\n",
    "        #     f.write(adapter_name)\n",
    "        #     f.write(trainer.evaluate(TAPT_dataset['test']))\n",
    "        #     f.write('\\n')\n",
    "        #     f.close()\n",
    "            \n",
    "        #     model.save_pretrained(f\"{adapter_name}_DAPT_TAPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=5e-4,\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     logging_steps=100,\n",
    "#     output_dir=\"./training_output/pretraining/DAPT\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=True,\n",
    "#     evaluation_strategy = 'steps',\n",
    "#     # load_best_model_at_end = True,\n",
    "#     save_steps = 100,\n",
    "#     gradient_accumulation_steps = 64,\n",
    "#     warmup_ratio = 0.06,\n",
    "#     weight_decay=0.01,\n",
    "#     adam_epsilon = 1e-6,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining_loop(num_models = 5, \n",
    "#                  training_args = training_args, \n",
    "#                 #  dataset = DAPT_dataset, TODO: Need to add DAPT training set\n",
    "#                  data_collator = data_collator, \n",
    "#                  adapter_name = \"DAPT_sci-erc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT+TAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=1'>2</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=2'>3</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=3'>4</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=4'>5</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=5'>6</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=6'>7</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./training_output/pretraining/TAPT\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=7'>8</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=8'>9</a>\u001b[0m     remove_unused_columns\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=9'>10</a>\u001b[0m     evaluation_strategy \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=10'>11</a>\u001b[0m     \u001b[39m# load_best_model_at_end = True,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=11'>12</a>\u001b[0m     save_steps \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=12'>13</a>\u001b[0m     gradient_accumulation_steps \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=13'>14</a>\u001b[0m     warmup_ratio \u001b[39m=\u001b[39m \u001b[39m0.06\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=14'>15</a>\u001b[0m     \u001b[39m# load_best_model_at_end = True,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=15'>16</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=16'>17</a>\u001b[0m     adam_epsilon \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=17'>18</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=0.0001,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/pretraining/TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    warmup_ratio = 0.06,\n",
    "    # load_best_model_at_end = True,\n",
    "    weight_decay=0.01,\n",
    "    adam_epsilon = 1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1688\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 600\n",
      "  1%|          | 4/600 [00:39<1:38:34,  9.92s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=0'>1</a>\u001b[0m pretraining_loop(num_models \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=1'>2</a>\u001b[0m                  training_args \u001b[39m=\u001b[39;49m training_args, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=2'>3</a>\u001b[0m                  dataset \u001b[39m=\u001b[39;49m dataset_pretraining, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=3'>4</a>\u001b[0m                  data_collator \u001b[39m=\u001b[39;49m data_collator, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=4'>5</a>\u001b[0m                  adapter_name \u001b[39m=\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTAPT_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataset_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 14'\u001b[0m in \u001b[0;36mpretraining_loop\u001b[1;34m(num_models, training_args, dataset, data_collator, adapter_name)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=27'>28</a>\u001b[0m writer \u001b[39m=\u001b[39m TensorBoardCallback(writer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=29'>30</a>\u001b[0m trainer \u001b[39m=\u001b[39m AdapterTrainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=30'>31</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=31'>32</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=35'>36</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[writer] \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=36'>37</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=38'>39</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=40'>41</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtraining_args\u001b[39m.\u001b[39moutput_dir\u001b[39m}\u001b[39;00m\u001b[39m/evaulations.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=41'>42</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(adapter)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1377\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1373'>1374</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1374'>1375</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1376'>1377</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1377'>1378</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1378'>1379</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1379'>1380</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1380'>1381</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1381'>1382</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1382'>1383</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1383'>1384</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretraining_loop(num_models = 2, \n",
    "                 training_args = training_args, \n",
    "                 dataset = dataset_pretraining, \n",
    "                 data_collator = data_collator, \n",
    "                 adapter_name = f\"TAPT_{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('f1')\n",
    "\n",
    "def compute_metric(EvalPrediction):\n",
    "  \n",
    "  logits, labels = EvalPrediction\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return metric.compute(predictions=predictions, references=labels, average= 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuning_loop(num_models, training_args, dataset, adapter_name, num_labels, load_adapter = False, adapter_dir = 'Path'):\n",
    "    \"\"\"The loop for finetuning num_models number of models to account for run2run variance\n",
    "\n",
    "    Args:\n",
    "        num_models (int): Number of models to loop through\n",
    "        training_args (transformers.TrainingArguments): The arguments to pass to the trainer\n",
    "        dataset (dataset): The dataset to train on\n",
    "        adapter_name (str): Name of the adapter to create/load\n",
    "        num_labels (int): Number of labels for classification task\n",
    "        load_adapter (bool, optional): Whether to load the adapter based on adapter_name. Defaults to False.\n",
    "        adapter_dir (str, optional): Path to the adapter to load when load_adapter is True. Defaults to 'Path'.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter, num_lables = num_labels, pretraining=False, load_adapter = load_adapter, adapter_dir = f\"{adapter_dir}/{adapter}\")\n",
    "        \n",
    "        writer = SummaryWriter(log_dir= f'runs/{adapter}')\n",
    "        writer = TensorBoardCallback(writer)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            callbacks=[writer],\n",
    "            compute_metrics = compute_metric \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        f = open(f\"{training_args.output_dir}/evaulations.txt\", \"a\")\n",
    "        f.write(adapter)\n",
    "        f.write(json.dumps(trainer.evaluate(dataset['test'])))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        # model.save_pretrained(f\"{adapter_name}\")\n",
    "        model.save_all_adapters(training_args.output_dir)\n",
    "        \n",
    "        trainer.remove_callback(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=1e-4,\n",
    "#     num_train_epochs=50,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     logging_steps=100,\n",
    "#     output_dir=\"./training_output/finetuning/DAPT\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    "#     evaluation_strategy = 'epoch',\n",
    "#     # load_best_model_at_end = True,\n",
    "#     save_steps = 100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning_loop(num_models = 5, \n",
    "#                  training_args = training_args, \n",
    "#                  dataset = scierc_dataset_finetuning,  \n",
    "#                  adapter_name = \"DAPT_sci-erc\",\n",
    "#                  load_adapter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT+TAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=1e-4,\n",
    "#     num_train_epochs=50,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     logging_steps=10,\n",
    "#     output_dir=\"./training_output/finetuning/DAPT_TAPT\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    "#     evaluation_strategy = 'epoch',\n",
    "#     # load_best_model_at_end = True,\n",
    "#     save_steps = 100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning_loop(num_models = 5, \n",
    "#                  training_args = training_args, \n",
    "#                  dataset = scierc_dataset_finetuning,  \n",
    "#                  adapter_name = \"DAPT_TAPT_sci-erc\",\n",
    "#                  load_adapter = True,\n",
    "#                  adapter_dir = \"./training_output/pretraining/DAPT_TAPT\",\n",
    "#                  num_labels = num_of_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=50,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     logging_steps=100,\n",
    "#     output_dir=\"./training_output/finetuning/TAPT\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    "#     evaluation_strategy = 'epoch',\n",
    "#     # load_best_model_at_end = True,\n",
    "#     save_steps = 100,\n",
    "#     lr_scheduler_type = 'constant',\n",
    "#     log_level  = 'error'\n",
    "    \n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output/finetuning/TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100,\n",
    "    # lr_scheduler_type = 'constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 2, \n",
    "                 training_args = training_args, \n",
    "                 dataset = dataset_finetuning,  \n",
    "                 adapter_name = dataset_name,\n",
    "                 load_adapter = True,\n",
    "                 adapter_dir = \"./training_output/pretraining/TAPT\",\n",
    "                 num_labels = num_of_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output/finetuning/No_Pretrain\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100,\n",
    "    # lr_scheduler_type = 'constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 2, \n",
    "                 training_args = training_args, \n",
    "                 dataset = dataset_finetuning,  \n",
    "                 adapter_name = dataset_name,\n",
    "                 load_adapter = False,\n",
    "                 num_labels = num_of_labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469684d7fc26b35c5046a1b9c559332af356cccfef9c5dca4231e653832987c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
