{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U adapter-transformers\n",
    "# !conda install -y -c conda-forge tensorboard\n",
    "# !conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration nsusemiehl--SciERC-f57c64a52b9c80c0\n",
      "Reusing dataset json (C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 3/3 [00:00<00:00, 999.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3219, 'test': 974, 'validation': 455}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "scierc_name = 'nsusemiehl/SciERC'\n",
    "scierc_dataset = load_dataset(scierc_name)\n",
    "print(scierc_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The agreement in question involves number in [[ nouns ]] and << reflexive pronouns >> and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .',\n",
       " 'label': 'CONJUNCTION',\n",
       " 'metadata': [7, 7, 9, 10]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scierc_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  4.00ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.58ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.74ba/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  7.07ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.81ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.17ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "# Encode the input data\n",
    "scierc_dataset = scierc_dataset.map(encode_batch, \n",
    "                                    batched=True, \n",
    "                                    remove_columns=scierc_dataset['train'].column_names, \n",
    "                                    )\n",
    "\n",
    "# Main data processing function that will concatenate all texts from\n",
    "# our dataset and generate chunks of block_size.\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "  \n",
    "scierc_dataset = scierc_dataset.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"dict\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_Roberta_DefaultAdapter.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_Roberta_DefaultAdapter.ipynb#ch0000008?line=0'>1</a>\u001b[0m {k: \u001b[39msum\u001b[39m(scierc_dataset[k], []) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m scierc_dataset\u001b[39m.\u001b[39mkeys()}\n",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_Roberta_DefaultAdapter.ipynb Cell 6'\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_Roberta_DefaultAdapter.ipynb#ch0000008?line=0'>1</a>\u001b[0m {k: \u001b[39msum\u001b[39;49m(scierc_dataset[k], []) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m scierc_dataset\u001b[39m.\u001b[39mkeys()}\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"dict\") to list"
     ]
    }
   ],
   "source": [
    "{k: sum(scierc_dataset[k], []) for k in scierc_dataset.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaAdapterModel\n",
    "\n",
    "config = RobertaConfig.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    # num_labels=num_of_labels,\n",
    ")\n",
    "model = RobertaAdapterModel.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new adapter\n",
    "model.add_adapter(\"sci_erc\")\n",
    "# Add a matching language model head\n",
    "model.add_masked_lm_head(\n",
    "    \"sci_erc\",\n",
    ")\n",
    "# Activate the adapter\n",
    "model.set_active_adapters(\"sci_erc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/pretraining\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "\n",
    "# Adding Support for Tensorboard, supposedly you don't have to do this, but I find that it doesn't work\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "writer = SummaryWriter()\n",
    "writer = TensorBoardCallback(writer)\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=scierc_dataset[\"train\"],\n",
    "    eval_dataset=scierc_dataset[\"validation\"],\n",
    "     \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469684d7fc26b35c5046a1b9c559332af356cccfef9c5dca4231e653832987c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
