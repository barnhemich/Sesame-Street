{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "# %conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "# %pip install -U adapter-transformers\n",
    "# %conda install -y -c conda-forge tensorboard\n",
    "# %pip install optuna\n",
    "# %pip install tqdm\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration zapsdcn--citation_intent-0b0f6658161cc990\n",
      "Reusing dataset json (C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\zapsdcn--citation_intent-0b0f6658161cc990\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1498.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 1688, 'test': 139, 'validation': 114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset_name = 'nsusemiehl/SciERC'\n",
    "dataset_name = 'zapsdcn/citation_intent'\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset_name = 'citation_intent'\n",
    "\n",
    "print(dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( Bobrow et al. , 1977 ; Chu-Carroll , 1999 ) .',\n",
       " 'label': 'Future',\n",
       " 'metadata': {}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block creates dataset for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'metadata']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\zapsdcn--citation_intent-0b0f6658161cc990\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-69c4b714d94b638b.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.34ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 15.14ba/s]\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\zapsdcn--citation_intent-0b0f6658161cc990\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-c2962fc358ac477f.arrow\n",
      "100%|██████████| 1/1 [00:00<00:00, 39.96ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 47.58ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# Tokenize the set for the transformer\n",
    "def encode_batch_pretraining(batch):\n",
    "    \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Encode the input data\n",
    "# NOTE: num_proc does not seem to work, for some reason it can't find the tokenizer\n",
    "print(dataset['train'].column_names)\n",
    "dataset_pretraining = dataset.map(encode_batch_pretraining, batched=True, remove_columns=dataset['train'].column_names,)\n",
    "\n",
    "# We make the labels the same as the input as this is language learning \n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "  \n",
    "dataset_pretraining = dataset_pretraining.map(add_labels, batched=True)\n",
    "dataset_pretraining.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collater adds padding in the form of EOS tokens, makes data augmentations of random masking ('mlm_probability)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating the dataset for task finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Background' 'CompareOrContrast' 'Extends' 'Future' 'Motivation' 'Uses']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Finding the number of labels\n",
    "import numpy as np\n",
    "labels = np.unique(np.array(dataset['train']['label']))\n",
    "num_of_labels = labels.size\n",
    "\n",
    "print(labels)\n",
    "print(num_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 999/1688 [00:00<00:00, 14678.14ex/s]\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 2 named metadata expected length 1000 but got length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:2328\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=2326'>2327</a>\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=2327'>2328</a>\u001b[0m                 writer\u001b[39m.\u001b[39;49mwrite(example)\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=2328'>2329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_writer.py:441\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[1;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=438'>439</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=440'>441</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_writer.py:399\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=397'>398</a>\u001b[0m     batch_examples[col] \u001b[39m=\u001b[39m [row[\u001b[39m0\u001b[39m][col] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples]\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=398'>399</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=399'>400</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_writer.py:495\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=493'>494</a>\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=494'>495</a>\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_arrays(arrays, schema\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=495'>496</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\pyarrow\\table.pxi:1834\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\pyarrow\\table.pxi:1445\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\pyarrow\\error.pxi:99\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 2 named metadata expected length 1000 but got length 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000008?line=8'>9</a>\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mmap(encode_labels, remove_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000008?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000008?line=10'>11</a>\u001b[0m     dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(encode_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000008?line=11'>12</a>\u001b[0m dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\dataset_dict.py:438\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=434'>435</a>\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=435'>436</a>\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=436'>437</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=437'>438</a>\u001b[0m     {\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=438'>439</a>\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=439'>440</a>\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=440'>441</a>\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=441'>442</a>\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=442'>443</a>\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=443'>444</a>\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=444'>445</a>\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=445'>446</a>\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=446'>447</a>\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=447'>448</a>\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=448'>449</a>\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=449'>450</a>\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=450'>451</a>\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=451'>452</a>\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=452'>453</a>\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=453'>454</a>\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=454'>455</a>\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=455'>456</a>\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=456'>457</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=457'>458</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=458'>459</a>\u001b[0m     }\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=459'>460</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\dataset_dict.py:439\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=434'>435</a>\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=435'>436</a>\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=436'>437</a>\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=437'>438</a>\u001b[0m     {\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=438'>439</a>\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=439'>440</a>\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=440'>441</a>\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=441'>442</a>\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=442'>443</a>\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=443'>444</a>\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=444'>445</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=445'>446</a>\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=446'>447</a>\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=447'>448</a>\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=448'>449</a>\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=449'>450</a>\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=450'>451</a>\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=451'>452</a>\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=452'>453</a>\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=453'>454</a>\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=454'>455</a>\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=455'>456</a>\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=456'>457</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=457'>458</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=458'>459</a>\u001b[0m     }\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/dataset_dict.py?line=459'>460</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:1955\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1951'>1952</a>\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1953'>1954</a>\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1954'>1955</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1955'>1956</a>\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1956'>1957</a>\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1957'>1958</a>\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1958'>1959</a>\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1959'>1960</a>\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1960'>1961</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1961'>1962</a>\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1962'>1963</a>\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1963'>1964</a>\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1964'>1965</a>\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1965'>1966</a>\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1966'>1967</a>\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1967'>1968</a>\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1968'>1969</a>\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1969'>1970</a>\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1970'>1971</a>\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1971'>1972</a>\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1972'>1973</a>\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1973'>1974</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1974'>1975</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=1976'>1977</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:520\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=517'>518</a>\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=518'>519</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=519'>520</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=520'>521</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=521'>522</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=522'>523</a>\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:487\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=479'>480</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=480'>481</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=481'>482</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=482'>483</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=483'>484</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=484'>485</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=485'>486</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=486'>487</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=487'>488</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=488'>489</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/fingerprint.py?line=451'>452</a>\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/fingerprint.py?line=452'>453</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/fingerprint.py?line=453'>454</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/fingerprint.py?line=455'>456</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/fingerprint.py?line=457'>458</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/fingerprint.py?line=459'>460</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/fingerprint.py?line=461'>462</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:2362\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=2359'>2360</a>\u001b[0m \u001b[39mif\u001b[39;00m update_data:\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=2360'>2361</a>\u001b[0m     \u001b[39mif\u001b[39;00m writer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=2361'>2362</a>\u001b[0m         writer\u001b[39m.\u001b[39;49mfinalize()\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=2362'>2363</a>\u001b[0m     \u001b[39mif\u001b[39;00m tmp_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_dataset.py?line=2363'>2364</a>\u001b[0m         tmp_file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_writer.py:522\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[1;34m(self, close_stream)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=519'>520</a>\u001b[0m     \u001b[39m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=520'>521</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhkey_record \u001b[39m=\u001b[39m []\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_examples_on_file()\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=523'>524</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema:\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_writer.py:399\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=395'>396</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m cols:\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=396'>397</a>\u001b[0m     \u001b[39m# Since current_examples contains (example, key) tuples\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=397'>398</a>\u001b[0m     batch_examples[col] \u001b[39m=\u001b[39m [row[\u001b[39m0\u001b[39m][col] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples]\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=398'>399</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_batch(batch_examples\u001b[39m=\u001b[39;49mbatch_examples)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=399'>400</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_examples \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\datasets\\arrow_writer.py:495\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=492'>493</a>\u001b[0m     inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=493'>494</a>\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[1;32m--> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=494'>495</a>\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_arrays(arrays, schema\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/datasets/arrow_writer.py?line=495'>496</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\pyarrow\\table.pxi:1834\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\pyarrow\\table.pxi:1445\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\pyarrow\\error.pxi:99\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 2 named metadata expected length 1000 but got length 0"
     ]
    }
   ],
   "source": [
    "# encoding the labels\n",
    "def encode_labels(dataset):\n",
    "    for i in range(num_of_labels):\n",
    "        if dataset['label'] == labels[i]:\n",
    "            dataset['label'] = i\n",
    "    return dataset\n",
    "\n",
    "if dataset_name == 'zapsdcn/citation_intent':\n",
    "    dataset = dataset.map(encode_labels, remove_columns=[\"metadata\"])\n",
    "else:\n",
    "    dataset = dataset.map(encode_labels)\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.82ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.26ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 27.00ba/s]\n"
     ]
    }
   ],
   "source": [
    "def encode_batch_finetuning(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Encode the input data\n",
    "dataset_finetuning = dataset.map(encode_batch_finetuning, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "dataset_finetuning = dataset_finetuning.rename_column(\"label\", 'labels')\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "dataset_finetuning.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaAdapterModel\n",
    "\n",
    "def model_init(adapter_name = 'default_adapter', \n",
    "               num_lables = 0, \n",
    "               pretraining = False,\n",
    "               load_adapter = False,\n",
    "               adapter_dir = 'path'):\n",
    "    \"\"\"Creates a new roBERTa model with the given name for its adapter.\n",
    "\n",
    "    Args:\n",
    "        adapter_name (str): The name of the adapter to load/create. Defaults to 'default_adapter'.\n",
    "        num_lables (int, optional): The number of labels for classification task. Defaults to 0.\n",
    "        pretraining (bool, optional): Whether to create a model for pretraining or classification. Defaults to False.\n",
    "        load_adapter (bool, optional): Whether to load an adapter with the adapter_name given or create a new one. Defaults to False.\n",
    "        adapter_dir (str, optional): Directory to load the adapter. If load_adapter you need to specify this.  Defaults to 'path'.\n",
    "\n",
    "    Returns:\n",
    "        RobertaAdapterModel: A roBERTA model with an adapter added to it.\n",
    "    \"\"\"\n",
    "    \n",
    "    if pretraining:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            # num_labels=num_of_labels,*-8536.22.03\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        if load_adapter:\n",
    "            # Add new adapter\n",
    "            model.load_adapter(adapter_dir)\n",
    "\n",
    "        else:\n",
    "            # Add new adapter\n",
    "            model.add_adapter(adapter_name)\n",
    "            \n",
    "        # Add a matching classification head\n",
    "        model.add_masked_lm_head(adapter_name)\n",
    "            \n",
    "    else:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            num_labels=num_lables,\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        if load_adapter:\n",
    "            # Add new adapter\n",
    "            model.load_adapter(adapter_dir)\n",
    "\n",
    "        else:\n",
    "            # Add new adapter\n",
    "            model.add_adapter(adapter_name)\n",
    "            \n",
    "        # Add a matching classification head\n",
    "        model.add_classification_head(\n",
    "                adapter_name,\n",
    "                num_labels=num_lables,\n",
    "                id2label={0:'COMPARE', 1:'CONJUNCTION', 2:'EVALUATE-FOR', \n",
    "                        3:'FEATURE-OF', 4:'HYPONYM-OF', 5:'PART-OF', 6:'USED-FOR'},\n",
    "                overwrite_ok = True)\n",
    "            \n",
    "    # Activate the adapter\n",
    "    model.train_adapter(adapter_name)    \n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer\n",
    "from datasets import load_metric\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "import json\n",
    "\n",
    "def pretraining_loop(num_models, training_args, dataset, \n",
    "                     data_collator, adapter_name, \n",
    "                    #  DAPT_n_TAPT, TAPT_dataset\n",
    "                     ):\n",
    "    \"\"\"The Loop for running num_models number of models to account for run2run variance. Will run the model \n",
    "        and evaluate.\n",
    "\n",
    "    Args:\n",
    "        num_models (int): Number of models to loop through\n",
    "        training_args (transformers.TrainingArguments): The arguments to pass to the trainer\n",
    "        dataset (dataset): The dataset to train on\n",
    "        data_collator (data_collator): The data collator for the trainer to use\n",
    "        adapter_name (str): Name of the adapter to create\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter, pretraining=True)\n",
    "        \n",
    "        writer = SummaryWriter(log_dir= f'runs/{adapter}')\n",
    "        writer = TensorBoardCallback(writer)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            data_collator=data_collator,  \n",
    "            callbacks=[writer] \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        f = open(f\"{training_args.output_dir}/evaulations.txt\", \"a\")\n",
    "        f.write(adapter)\n",
    "        f.write(json.dumps(trainer.evaluate(dataset['test'])))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        model.save_all_adapters(training_args.output_dir, with_head=False)\n",
    "        # model.save_pretrained(f\"{adapter_name}\")\n",
    "        \n",
    "        # if DAPT_n_TAPT:\n",
    "        #     trainer = AdapterTrainer(\n",
    "        #         model=model,\n",
    "        #         args=training_args,\n",
    "        #         train_dataset=TAPT_dataset[\"train\"],\n",
    "        #         eval_dataset=TAPT_dataset[\"validation\"],\n",
    "        #         data_collator=data_collator,  \n",
    "        #         callbacks=[writer] \n",
    "        #     )\n",
    "            \n",
    "        #     trainer.train()\n",
    "        \n",
    "        #     f = open(\"DAPT_TAPT_evaulations.txt\", \"a\")\n",
    "        #     f.write(adapter_name)\n",
    "        #     f.write(trainer.evaluate(TAPT_dataset['test']))\n",
    "        #     f.write('\\n')\n",
    "        #     f.close()\n",
    "            \n",
    "        #     model.save_pretrained(f\"{adapter_name}_DAPT_TAPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=5e-4,\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     logging_steps=100,\n",
    "#     output_dir=\"./training_output/pretraining/DAPT\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=True,\n",
    "#     evaluation_strategy = 'steps',\n",
    "#     # load_best_model_at_end = True,\n",
    "#     save_steps = 100,\n",
    "#     gradient_accumulation_steps = 64,\n",
    "#     warmup_ratio = 0.06,\n",
    "#     weight_decay=0.01,\n",
    "#     adam_epsilon = 1e-6,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining_loop(num_models = 5, \n",
    "#                  training_args = training_args, \n",
    "#                 #  dataset = DAPT_dataset, TODO: Need to add DAPT training set\n",
    "#                  data_collator = data_collator, \n",
    "#                  adapter_name = \"DAPT_sci-erc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT+TAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=1'>2</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=2'>3</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=3'>4</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=4'>5</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=5'>6</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=6'>7</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./training_output/pretraining/TAPT\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=7'>8</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=8'>9</a>\u001b[0m     remove_unused_columns\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=9'>10</a>\u001b[0m     evaluation_strategy \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=10'>11</a>\u001b[0m     \u001b[39m# load_best_model_at_end = True,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=11'>12</a>\u001b[0m     save_steps \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=12'>13</a>\u001b[0m     gradient_accumulation_steps \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=13'>14</a>\u001b[0m     warmup_ratio \u001b[39m=\u001b[39m \u001b[39m0.06\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=14'>15</a>\u001b[0m     \u001b[39m# load_best_model_at_end = True,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=15'>16</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=16'>17</a>\u001b[0m     adam_epsilon \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000020?line=17'>18</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=0.0001,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/pretraining/TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    warmup_ratio = 0.06,\n",
    "    # load_best_model_at_end = True,\n",
    "    weight_decay=0.01,\n",
    "    adam_epsilon = 1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1688\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 600\n",
      "  1%|          | 4/600 [00:39<1:38:34,  9.92s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=0'>1</a>\u001b[0m pretraining_loop(num_models \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=1'>2</a>\u001b[0m                  training_args \u001b[39m=\u001b[39;49m training_args, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=2'>3</a>\u001b[0m                  dataset \u001b[39m=\u001b[39;49m dataset_pretraining, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=3'>4</a>\u001b[0m                  data_collator \u001b[39m=\u001b[39;49m data_collator, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000021?line=4'>5</a>\u001b[0m                  adapter_name \u001b[39m=\u001b[39;49m \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTAPT_\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataset_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 14'\u001b[0m in \u001b[0;36mpretraining_loop\u001b[1;34m(num_models, training_args, dataset, data_collator, adapter_name)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=27'>28</a>\u001b[0m writer \u001b[39m=\u001b[39m TensorBoardCallback(writer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=29'>30</a>\u001b[0m trainer \u001b[39m=\u001b[39m AdapterTrainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=30'>31</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=31'>32</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=35'>36</a>\u001b[0m     callbacks\u001b[39m=\u001b[39m[writer] \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=36'>37</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=38'>39</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=40'>41</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtraining_args\u001b[39m.\u001b[39moutput_dir\u001b[39m}\u001b[39;00m\u001b[39m/evaulations.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000013?line=41'>42</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(adapter)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1377\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1373'>1374</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1374'>1375</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1376'>1377</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1377'>1378</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1378'>1379</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1379'>1380</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1380'>1381</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1381'>1382</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1382'>1383</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1383'>1384</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretraining_loop(num_models = 2, \n",
    "                 training_args = training_args, \n",
    "                 dataset = dataset_pretraining, \n",
    "                 data_collator = data_collator, \n",
    "                 adapter_name = f\"TAPT_{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('f1')\n",
    "\n",
    "def compute_metric(EvalPrediction):\n",
    "  \n",
    "  logits, labels = EvalPrediction\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return metric.compute(predictions=predictions, references=labels, average= 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuning_loop(num_models, training_args, dataset, adapter_name, num_labels, load_adapter = False, adapter_dir = 'Path'):\n",
    "    \"\"\"The loop for finetuning num_models number of models to account for run2run variance\n",
    "\n",
    "    Args:\n",
    "        num_models (int): Number of models to loop through\n",
    "        training_args (transformers.TrainingArguments): The arguments to pass to the trainer\n",
    "        dataset (dataset): The dataset to train on\n",
    "        adapter_name (str): Name of the adapter to create/load\n",
    "        num_labels (int): Number of labels for classification task\n",
    "        load_adapter (bool, optional): Whether to load the adapter based on adapter_name. Defaults to False.\n",
    "        adapter_dir (str, optional): Path to the adapter to load when load_adapter is True. Defaults to 'Path'.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter, num_lables = num_labels, pretraining=False, load_adapter = load_adapter, adapter_dir = f\"{adapter_dir}/{adapter}\")\n",
    "        \n",
    "        writer = SummaryWriter(log_dir= f'runs/{adapter}')\n",
    "        writer = TensorBoardCallback(writer)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            callbacks=[writer],\n",
    "            compute_metrics = compute_metric \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        f = open(f\"{training_args.output_dir}/evaulations.txt\", \"a\")\n",
    "        f.write(adapter)\n",
    "        f.write(json.dumps(trainer.evaluate(dataset['test'])))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        # model.save_pretrained(f\"{adapter_name}\")\n",
    "        model.save_all_adapters(training_args.output_dir)\n",
    "        \n",
    "        trainer.remove_callback(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=1e-4,\n",
    "#     num_train_epochs=50,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     logging_steps=100,\n",
    "#     output_dir=\"./training_output/finetuning/DAPT\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    "#     evaluation_strategy = 'epoch',\n",
    "#     # load_best_model_at_end = True,\n",
    "#     save_steps = 100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning_loop(num_models = 5, \n",
    "#                  training_args = training_args, \n",
    "#                  dataset = scierc_dataset_finetuning,  \n",
    "#                  adapter_name = \"DAPT_sci-erc\",\n",
    "#                  load_adapter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT+TAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=1e-4,\n",
    "#     num_train_epochs=50,\n",
    "#     per_device_train_batch_size=32,\n",
    "#     per_device_eval_batch_size=32,\n",
    "#     logging_steps=10,\n",
    "#     output_dir=\"./training_output/finetuning/DAPT_TAPT\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    "#     evaluation_strategy = 'epoch',\n",
    "#     # load_best_model_at_end = True,\n",
    "#     save_steps = 100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning_loop(num_models = 5, \n",
    "#                  training_args = training_args, \n",
    "#                  dataset = scierc_dataset_finetuning,  \n",
    "#                  adapter_name = \"DAPT_TAPT_sci-erc\",\n",
    "#                  load_adapter = True,\n",
    "#                  adapter_dir = \"./training_output/pretraining/DAPT_TAPT\",\n",
    "#                  num_labels = num_of_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=50,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     logging_steps=100,\n",
    "#     output_dir=\"./training_output/finetuning/TAPT\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    "#     evaluation_strategy = 'epoch',\n",
    "#     # load_best_model_at_end = True,\n",
    "#     save_steps = 100,\n",
    "#     lr_scheduler_type = 'constant',\n",
    "#     log_level  = 'error'\n",
    "    \n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output/finetuning/TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100,\n",
    "    # lr_scheduler_type = 'constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"TAPT_sci-erc\",\n",
    "                 load_adapter = True,\n",
    "                 adapter_dir = \"./training_output/pretraining/TAPT\",\n",
    "                 num_labels = num_of_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output/finetuning/No_Pretrain\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100,\n",
    "    # lr_scheduler_type = 'constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"sci-erc\",\n",
    "                 load_adapter = False,\n",
    "                 num_labels = num_of_labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469684d7fc26b35c5046a1b9c559332af356cccfef9c5dca4231e653832987c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
