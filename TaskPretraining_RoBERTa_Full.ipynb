{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "%conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "%pip install -U adapter-transformers\n",
    "%conda install -y -c conda-forge tensorboard\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "scierc_name = 'nsusemiehl/SciERC'\n",
    "scierc_dataset = load_dataset(scierc_name)\n",
    "print(scierc_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scierc_dataset['train'][255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block creates dataset for TAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch_pretraining(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Encode the input data\n",
    "# NOTE: num_proc does not seem to work, for some reason it can't find the tokenizer\n",
    "scierc_dataset_pretraining = scierc_dataset.map(encode_batch_pretraining, \n",
    "                                    batched=True, \n",
    "                                    remove_columns=scierc_dataset['train'].column_names, \n",
    "                                    )\n",
    "\n",
    "def add_labels(examples):\n",
    "  examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "  return examples\n",
    "  \n",
    "scierc_dataset_pretraining = scierc_dataset_pretraining.map(add_labels, batched=True)\n",
    "scierc_dataset_pretraining.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collater adds padding in the form of EOS tokens, makes data augmentations of random masking ('mlm_probability)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating the dataset for task finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of labels\n",
    "import numpy as np\n",
    "labels = np.unique(np.array(scierc_dataset['train']['label']))\n",
    "num_of_labels = labels.size\n",
    "\n",
    "print(labels)\n",
    "print(num_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the labels\n",
    "def encode_labels(dataset):\n",
    "    for i in range(num_of_labels):\n",
    "        if dataset['label'] == labels[i]:\n",
    "            dataset['label'] = i\n",
    "    return dataset\n",
    "\n",
    "scierc_dataset = scierc_dataset.map(encode_labels)\n",
    "scierc_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch_finetuning(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Encode the input data\n",
    "scierc_dataset_finetuning = scierc_dataset.map(encode_batch_finetuning, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "scierc_dataset_finetuning = scierc_dataset_finetuning.rename_column(\"label\", 'labels')\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "scierc_dataset_finetuning.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaAdapterModel\n",
    "\n",
    "def model_init(adapter_name = 'default_adapter', \n",
    "               num_lables = 0, \n",
    "               pretraining = False,\n",
    "               load_adapter = False):\n",
    "    \n",
    "    if pretraining:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            # num_labels=num_of_labels,\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        if load_adapter:\n",
    "            # Add new adapter\n",
    "            model.load_adapter(adapter_name)\n",
    "\n",
    "        else:\n",
    "            # Add new adapter\n",
    "            model.add_adapter(adapter_name)\n",
    "            \n",
    "        # Add a matching classification head\n",
    "        model.add_masked_lm_head(adapter_name)\n",
    "            \n",
    "    else:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            num_labels=num_of_labels,\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        if load_adapter:\n",
    "            # Add new adapter\n",
    "            model.load_adapter(adapter_name)\n",
    "\n",
    "        else:\n",
    "            # Add new adapter\n",
    "            model.add_adapter(adapter_name)\n",
    "            \n",
    "        # Add a matching classification head\n",
    "        model.add_classification_head(\n",
    "                adapter_name,\n",
    "                num_labels=num_lables,\n",
    "                id2label={0:'COMPARE', 1:'CONJUNCTION', 2:'EVALUATE-FOR', \n",
    "                        3:'FEATURE-OF', 4:'HYPONYM-OF', 5:'PART-OF', 6:'USED-FOR'}\n",
    "                )\n",
    "            \n",
    "    # Activate the adapter\n",
    "    model.train_adapter(adapter_name)    \n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer\n",
    "from datasets import load_metric\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "def pretraining_loop(num_models, training_args, dataset, \n",
    "                     data_collator, adapter_name, DAPT_n_TAPT, \n",
    "                     TAPT_dataset):\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter_name = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter_name, pretraining=True)\n",
    "        \n",
    "        writer = SummaryWriter()\n",
    "        writer = TensorBoardCallback(writer, filename_suffix = adapter_name)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            data_collator=data_collator,  \n",
    "            callbacks=[writer] \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        f = open(\"evaulations.txt\", \"a\")\n",
    "        f.write(adapter_name)\n",
    "        f.write(trainer.evaluate(dataset['test']))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        model.save_pretrained(f\"{adapter_name}\")\n",
    "        \n",
    "        # if DAPT_n_TAPT:\n",
    "        #     trainer = AdapterTrainer(\n",
    "        #         model=model,\n",
    "        #         args=training_args,\n",
    "        #         train_dataset=TAPT_dataset[\"train\"],\n",
    "        #         eval_dataset=TAPT_dataset[\"validation\"],\n",
    "        #         data_collator=data_collator,  \n",
    "        #         callbacks=[writer] \n",
    "        #     )\n",
    "            \n",
    "        #     trainer.train()\n",
    "        \n",
    "        #     f = open(\"DAPT_TAPT_evaulations.txt\", \"a\")\n",
    "        #     f.write(adapter_name)\n",
    "        #     f.write(trainer.evaluate(TAPT_dataset['test']))\n",
    "        #     f.write('\\n')\n",
    "        #     f.close()\n",
    "            \n",
    "        #     model.save_pretrained(f\"{adapter_name}_DAPT_TAPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/pretraining/DAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                #  dataset = DAPT_dataset, TODO: Need to add DAPT training set\n",
    "                 data_collator = data_collator, \n",
    "                 adapter_name = \"DAPT_sci-erc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT+TAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/pretraining/TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_pretraining, \n",
    "                 data_collator = data_collator, \n",
    "                 adapter_name = \"TAPT_sci-erc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuning_loop(num_models, training_args, dataset, adapter_name, load_adapter = False):\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter_name = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter_name, pretraining=False, load_adapter = load_adapter)\n",
    "        \n",
    "        writer = SummaryWriter()\n",
    "        writer = TensorBoardCallback(writer, filename_suffix = adapter_name)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            data_collator=data_collator,  \n",
    "            callbacks=[writer] \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        f = open(\"evaulations.txt\", \"a\")\n",
    "        f.write(adapter_name)\n",
    "        f.write(trainer.evaluate(dataset['test']))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        model.save_pretrained(f\"{adapter_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/finetuning/DAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"DAPT_sci-erc\",\n",
    "                 load_adapter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT+TAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/finetuning/DAPT_TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"DAPT_TAPT_sci-erc\",\n",
    "                 load_adapter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/finetuning/TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"TAPT_sci-erc\",\n",
    "                 load_adapter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/finetuning/No_Pretrain\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"sci-erc\",\n",
    "                 load_adapter = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469684d7fc26b35c5046a1b9c559332af356cccfef9c5dca4231e653832987c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
