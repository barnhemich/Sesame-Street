{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "%conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "%pip install -U adapter-transformers\n",
    "%conda install -y -c conda-forge tensorboard\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "scierc_name = 'nsusemiehl/SciERC'\n",
    "scierc_dataset = load_dataset(scierc_name)\n",
    "print(scierc_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scierc_dataset['train'][255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block creates dataset for TAPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def encode_batch_pretraining(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Encode the input data\n",
    "# NOTE: num_proc does not seem to work, for some reason it can't find the tokenizer\n",
    "scierc_dataset_pretraining = scierc_dataset.map(encode_batch_pretraining, \n",
    "                                    batched=True, \n",
    "                                    remove_columns=scierc_dataset['train'].column_names, \n",
    "                                    )\n",
    "\n",
    "def add_labels(examples):\n",
    "  examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "  return examples\n",
    "  \n",
    "scierc_dataset_pretraining = scierc_dataset_pretraining.map(add_labels, batched=True)\n",
    "scierc_dataset_pretraining.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collater adds padding in the form of EOS tokens, makes data augmentations of random masking ('mlm_probability)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating the dataset for task finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of labels\n",
    "import numpy as np\n",
    "labels = np.unique(np.array(scierc_dataset['train']['label']))\n",
    "num_of_labels = labels.size\n",
    "\n",
    "print(labels)\n",
    "print(num_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding the labels\n",
    "def encode_labels(dataset):\n",
    "    for i in range(num_of_labels):\n",
    "        if dataset['label'] == labels[i]:\n",
    "            dataset['label'] = i\n",
    "    return dataset\n",
    "\n",
    "scierc_dataset = scierc_dataset.map(encode_labels)\n",
    "scierc_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch_finetuning(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Encode the input data\n",
    "scierc_dataset_finetuning = scierc_dataset.map(encode_batch_finetuning, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "scierc_dataset_finetuning = scierc_dataset_finetuning.rename_column(\"label\", 'labels')\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "scierc_dataset_finetuning.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaAdapterModel\n",
    "\n",
    "def model_init(adapter_name = 'default_adapter', \n",
    "               num_lables = 0, \n",
    "               pretraining = False):\n",
    "    \n",
    "    if pretraining:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            # num_labels=num_of_labels,\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        # Add new adapter\n",
    "        model.add_adapter(adapter_name)\n",
    "        # Add a matching classification head\n",
    "        model.add_masked_lm_head(adapter_name)\n",
    "        # Activate the adapter\n",
    "        model.train_adapter(adapter_name)\n",
    "    \n",
    "    else: \n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            num_labels=num_of_labels,\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        # Add new adapter\n",
    "        model.add_adapter(adapter_name)\n",
    "        # Add a matching classification head\n",
    "        model.add_classification_head(\n",
    "            adapter_name,\n",
    "            num_labels=num_of_labels,\n",
    "            id2label={0:'COMPARE', 1:'CONJUNCTION', 2:'EVALUATE-FOR', \n",
    "                    3:'FEATURE-OF', 4:'HYPONYM-OF', 5:'PART-OF', 6:'USED-FOR'}\n",
    "        )\n",
    "        # Activate the adapter\n",
    "        model.train_adapter(adapter_name)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer\n",
    "from datasets import load_metric\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "def pretraining_loop(num_models, training_args, dataset, data_collator, adapter_name):\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter_name = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter_name, pretraining=True)\n",
    "        \n",
    "        writer = SummaryWriter()\n",
    "        writer = TensorBoardCallback(writer, filename_suffix = adapter_name)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            data_collator=data_collator,  \n",
    "            callbacks=[writer] \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        model.save_pretrained(adapter_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=1'>2</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=2'>3</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=3'>4</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=4'>5</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=5'>6</a>\u001b[0m     logging_steps\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=6'>7</a>\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./training_output/pretraining/DAPT\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=7'>8</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=8'>9</a>\u001b[0m     \u001b[39m# The next line is important to ensure the dataset labels are properly passed to the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=9'>10</a>\u001b[0m     remove_unused_columns\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=10'>11</a>\u001b[0m     evaluation_strategy \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=11'>12</a>\u001b[0m     \u001b[39m# load_best_model_at_end = True,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=12'>13</a>\u001b[0m     save_steps \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000026?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/pretraining/DAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                #  dataset = DAPT_dataset, \n",
    "                 data_collator = data_collator, \n",
    "                 adapter_name = \"DAPT_sci-erc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer\n",
    "from datasets import load_metric\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/pretraining\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "\n",
    "metric = load_metric('f1')\n",
    "\n",
    "def compute_metric(EvalPrediction):\n",
    "  \n",
    "  logits, labels = EvalPrediction\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return metric.compute(predictions=predictions, references=labels, average= 'macro')\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=scierc_dataset[\"train\"],\n",
    "    eval_dataset=scierc_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metric = compute_metric\n",
    "     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(scierc_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469684d7fc26b35c5046a1b9c559332af356cccfef9c5dca4231e653832987c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
