{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Libraries\n",
    "# %conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n",
    "%pip install -U adapter-transformers\n",
    "%conda install -y -c conda-forge tensorboard\n",
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration nsusemiehl--SciERC-f57c64a52b9c80c0\n",
      "Reusing dataset json (C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 3/3 [00:00<00:00, 999.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 3219, 'test': 974, 'validation': 455}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "scierc_name = 'nsusemiehl/SciERC'\n",
    "scierc_dataset = load_dataset(scierc_name)\n",
    "print(scierc_dataset.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'We present two [[ methods ]] for capturing << nonstationary chaos >> , then present a few examples including biological signals , ocean waves and traffic flow .',\n",
       " 'label': 'USED-FOR',\n",
       " 'metadata': [3, 3, 6, 7]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scierc_dataset['train'][255]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block creates dataset for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.63ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.18ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.04ba/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  7.55ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.17ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.98ba/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# Tokenize the set for the transformer\n",
    "def encode_batch_pretraining(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Encode the input data\n",
    "# NOTE: num_proc does not seem to work, for some reason it can't find the tokenizer\n",
    "scierc_dataset_pretraining = scierc_dataset.map(encode_batch_pretraining, \n",
    "                                    batched=True, \n",
    "                                    remove_columns=scierc_dataset['train'].column_names, \n",
    "                                    )\n",
    "\n",
    "# We make the labels the same as the input as this is language learning \n",
    "def add_labels(examples):\n",
    "  examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "  return examples\n",
    "  \n",
    "scierc_dataset_pretraining = scierc_dataset_pretraining.map(add_labels, batched=True)\n",
    "scierc_dataset_pretraining.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collater adds padding in the form of EOS tokens, makes data augmentations of random masking ('mlm_probability)\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are creating the dataset for task finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COMPARE' 'CONJUNCTION' 'EVALUATE-FOR' 'FEATURE-OF' 'HYPONYM-OF'\n",
      " 'PART-OF' 'USED-FOR']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Finding the number of labels\n",
    "import numpy as np\n",
    "labels = np.unique(np.array(scierc_dataset['train']['label']))\n",
    "num_of_labels = labels.size\n",
    "\n",
    "print(labels)\n",
    "print(num_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-3fa4decd4606a523.arrow\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-4d2e52dbe4cdbad6.arrow\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-214c6724dd02783d.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'The agreement in question involves number in [[ nouns ]] and << reflexive pronouns >> and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .',\n",
       " 'label': 1,\n",
       " 'metadata': [7, 7, 9, 10]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding the labels\n",
    "def encode_labels(dataset):\n",
    "    for i in range(num_of_labels):\n",
    "        if dataset['label'] == labels[i]:\n",
    "            dataset['label'] = i\n",
    "    return dataset\n",
    "\n",
    "scierc_dataset = scierc_dataset.map(encode_labels)\n",
    "scierc_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-9df92facd1c396b0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-c8f0959a9035f1ea.arrow\n",
      "Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-2d3aa4934791a9fc.arrow\n"
     ]
    }
   ],
   "source": [
    "def encode_batch_finetuning(batch):\n",
    "  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n",
    "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Encode the input data\n",
    "scierc_dataset_finetuning = scierc_dataset.map(encode_batch_finetuning, batched=True)\n",
    "# The transformers model expects the target class column to be named \"labels\"\n",
    "scierc_dataset_finetuning = scierc_dataset_finetuning.rename_column(\"label\", 'labels')\n",
    "# Transform to pytorch tensors and only output the required columns\n",
    "scierc_dataset_finetuning.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaAdapterModel\n",
    "\n",
    "def model_init(adapter_name = 'default_adapter', \n",
    "               num_lables = 0, \n",
    "               pretraining = False,\n",
    "               load_adapter = False,\n",
    "               adapter_dir = 'path'):\n",
    "    \n",
    "    if pretraining:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            # num_labels=num_of_labels,\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        if load_adapter:\n",
    "            # Add new adapter\n",
    "            model.load_adapter(adapter_dir)\n",
    "\n",
    "        else:\n",
    "            # Add new adapter\n",
    "            model.add_adapter(adapter_name)\n",
    "            \n",
    "        # Add a matching classification head\n",
    "        model.add_masked_lm_head(adapter_name)\n",
    "            \n",
    "    else:\n",
    "        config = RobertaConfig.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            num_labels=num_lables,\n",
    "        )\n",
    "        model = RobertaAdapterModel.from_pretrained(\n",
    "            \"roberta-base\",\n",
    "            config=config,\n",
    "        )\n",
    "        \n",
    "        if load_adapter:\n",
    "            # Add new adapter\n",
    "            model.load_adapter(adapter_dir)\n",
    "\n",
    "        else:\n",
    "            # Add new adapter\n",
    "            model.add_adapter(adapter_name)\n",
    "            \n",
    "        # Add a matching classification head\n",
    "        model.add_classification_head(\n",
    "                adapter_name,\n",
    "                num_labels=num_lables,\n",
    "                id2label={0:'COMPARE', 1:'CONJUNCTION', 2:'EVALUATE-FOR', \n",
    "                        3:'FEATURE-OF', 4:'HYPONYM-OF', 5:'PART-OF', 6:'USED-FOR'},\n",
    "                overwrite_ok = True)\n",
    "            \n",
    "    # Activate the adapter\n",
    "    model.train_adapter(adapter_name)    \n",
    "     \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer\n",
    "from datasets import load_metric\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "import json\n",
    "\n",
    "def pretraining_loop(num_models, training_args, dataset, \n",
    "                     data_collator, adapter_name, \n",
    "                    #  DAPT_n_TAPT, TAPT_dataset\n",
    "                     ):\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter, pretraining=True)\n",
    "        \n",
    "        writer = SummaryWriter(log_dir= f'runs/{adapter}')\n",
    "        writer = TensorBoardCallback(writer)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            data_collator=data_collator,  \n",
    "            callbacks=[writer] \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        f = open(f\"{training_args.output_dir}/evaulations.txt\", \"a\")\n",
    "        f.write(adapter)\n",
    "        f.write(json.dumps(trainer.evaluate(dataset['test'])))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        model.save_all_adapters(training_args.output_dir, with_head=False)\n",
    "        # model.save_pretrained(f\"{adapter_name}\")\n",
    "        \n",
    "        # if DAPT_n_TAPT:\n",
    "        #     trainer = AdapterTrainer(\n",
    "        #         model=model,\n",
    "        #         args=training_args,\n",
    "        #         train_dataset=TAPT_dataset[\"train\"],\n",
    "        #         eval_dataset=TAPT_dataset[\"validation\"],\n",
    "        #         data_collator=data_collator,  \n",
    "        #         callbacks=[writer] \n",
    "        #     )\n",
    "            \n",
    "        #     trainer.train()\n",
    "        \n",
    "        #     f = open(\"DAPT_TAPT_evaulations.txt\", \"a\")\n",
    "        #     f.write(adapter_name)\n",
    "        #     f.write(trainer.evaluate(TAPT_dataset['test']))\n",
    "        #     f.write('\\n')\n",
    "        #     f.close()\n",
    "            \n",
    "        #     model.save_pretrained(f\"{adapter_name}_DAPT_TAPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output/pretraining/DAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                #  dataset = DAPT_dataset, TODO: Need to add DAPT training set\n",
    "                 data_collator = data_collator, \n",
    "                 adapter_name = \"DAPT_sci-erc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT+TAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output/pretraining/TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3219\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2525\n",
      "  4%|▍         | 100/2525 [00:40<15:51,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.6483, 'learning_rate': 1.920792079207921e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  4%|▍         | 100/2525 [00:43<15:51,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-100\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 10.142413139343262, 'eval_runtime': 2.6539, 'eval_samples_per_second': 171.445, 'eval_steps_per_second': 5.652, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "  8%|▊         | 200/2525 [01:22<15:02,  2.58it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.6972, 'learning_rate': 1.841584158415842e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  8%|▊         | 200/2525 [01:24<15:02,  2.58it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-200\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.720895290374756, 'eval_runtime': 2.6124, 'eval_samples_per_second': 174.171, 'eval_steps_per_second': 5.742, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 12%|█▏        | 300/2525 [02:04<14:30,  2.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.6358, 'learning_rate': 1.7623762376237624e-05, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▏        | 300/2525 [02:07<14:30,  2.56it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-300\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.3029398918151855, 'eval_runtime': 2.6234, 'eval_samples_per_second': 173.44, 'eval_steps_per_second': 5.718, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 16%|█▌        | 400/2525 [02:46<13:50,  2.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.305, 'learning_rate': 1.683168316831683e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 16%|█▌        | 400/2525 [02:49<13:50,  2.56it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-400\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.018126487731934, 'eval_runtime': 2.5333, 'eval_samples_per_second': 179.607, 'eval_steps_per_second': 5.921, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 20%|█▉        | 500/2525 [03:28<13:12,  2.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0854, 'learning_rate': 1.6039603960396042e-05, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|█▉        | 500/2525 [03:30<13:12,  2.56it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-500\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.876257419586182, 'eval_runtime': 2.6344, 'eval_samples_per_second': 172.715, 'eval_steps_per_second': 5.694, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 24%|██▍       | 600/2525 [04:09<12:03,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.9367, 'learning_rate': 1.5247524752475249e-05, 'epoch': 5.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 24%|██▍       | 600/2525 [04:11<12:03,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-600\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.7529778480529785, 'eval_runtime': 2.6284, 'eval_samples_per_second': 173.11, 'eval_steps_per_second': 5.707, 'epoch': 5.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 28%|██▊       | 700/2525 [04:50<11:24,  2.67it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8105, 'learning_rate': 1.4455445544554456e-05, 'epoch': 6.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 28%|██▊       | 700/2525 [04:52<11:24,  2.67it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-700\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.650875091552734, 'eval_runtime': 2.5363, 'eval_samples_per_second': 179.395, 'eval_steps_per_second': 5.914, 'epoch': 6.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 32%|███▏      | 800/2525 [05:31<10:49,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7303, 'learning_rate': 1.3663366336633666e-05, 'epoch': 7.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 32%|███▏      | 800/2525 [05:33<10:49,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-800\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.583042621612549, 'eval_runtime': 2.5443, 'eval_samples_per_second': 178.83, 'eval_steps_per_second': 5.896, 'epoch': 7.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 36%|███▌      | 900/2525 [06:12<10:10,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6417, 'learning_rate': 1.2871287128712873e-05, 'epoch': 8.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 36%|███▌      | 900/2525 [06:14<10:10,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-900\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.502331256866455, 'eval_runtime': 2.5323, 'eval_samples_per_second': 179.678, 'eval_steps_per_second': 5.923, 'epoch': 8.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 40%|███▉      | 1000/2525 [06:54<09:57,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5813, 'learning_rate': 1.207920792079208e-05, 'epoch': 9.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|███▉      | 1000/2525 [06:57<09:57,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1000\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.433701992034912, 'eval_runtime': 2.5413, 'eval_samples_per_second': 179.041, 'eval_steps_per_second': 5.902, 'epoch': 9.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 44%|████▎     | 1100/2525 [07:37<09:19,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5278, 'learning_rate': 1.1287128712871288e-05, 'epoch': 10.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 44%|████▎     | 1100/2525 [07:39<09:19,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1100\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.3937201499938965, 'eval_runtime': 2.5363, 'eval_samples_per_second': 179.395, 'eval_steps_per_second': 5.914, 'epoch': 10.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 48%|████▊     | 1200/2525 [08:19<08:39,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4568, 'learning_rate': 1.0495049504950497e-05, 'epoch': 11.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 48%|████▊     | 1200/2525 [08:22<08:39,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1200\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.367732048034668, 'eval_runtime': 2.5463, 'eval_samples_per_second': 178.69, 'eval_steps_per_second': 5.891, 'epoch': 11.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 51%|█████▏    | 1300/2525 [09:00<07:41,  2.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4251, 'learning_rate': 9.702970297029704e-06, 'epoch': 12.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 51%|█████▏    | 1300/2525 [09:03<07:41,  2.65it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1300\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.3208537101745605, 'eval_runtime': 2.6044, 'eval_samples_per_second': 174.706, 'eval_steps_per_second': 5.76, 'epoch': 12.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 55%|█████▌    | 1400/2525 [09:42<07:03,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3785, 'learning_rate': 8.910891089108911e-06, 'epoch': 13.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 55%|█████▌    | 1400/2525 [09:44<07:03,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1400\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.2930803298950195, 'eval_runtime': 2.5373, 'eval_samples_per_second': 179.324, 'eval_steps_per_second': 5.912, 'epoch': 13.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 59%|█████▉    | 1500/2525 [10:23<06:25,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3612, 'learning_rate': 8.11881188118812e-06, 'epoch': 14.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 59%|█████▉    | 1500/2525 [10:25<06:25,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1500\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.248208045959473, 'eval_runtime': 2.5353, 'eval_samples_per_second': 179.466, 'eval_steps_per_second': 5.916, 'epoch': 14.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 63%|██████▎   | 1600/2525 [11:05<06:02,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3458, 'learning_rate': 7.326732673267327e-06, 'epoch': 15.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 63%|██████▎   | 1600/2525 [11:08<06:02,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1600\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.271334171295166, 'eval_runtime': 2.5383, 'eval_samples_per_second': 179.253, 'eval_steps_per_second': 5.909, 'epoch': 15.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 67%|██████▋   | 1700/2525 [11:46<05:10,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2776, 'learning_rate': 6.534653465346535e-06, 'epoch': 16.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 1700/2525 [11:49<05:10,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1700\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.216168403625488, 'eval_runtime': 2.5363, 'eval_samples_per_second': 179.395, 'eval_steps_per_second': 5.914, 'epoch': 16.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 71%|███████▏  | 1800/2525 [12:29<04:43,  2.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2502, 'learning_rate': 5.7425742574257425e-06, 'epoch': 17.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 71%|███████▏  | 1800/2525 [12:31<04:43,  2.56it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1800\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.191011428833008, 'eval_runtime': 2.6414, 'eval_samples_per_second': 172.257, 'eval_steps_per_second': 5.679, 'epoch': 17.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 75%|███████▌  | 1900/2525 [13:10<03:55,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2584, 'learning_rate': 4.950495049504951e-06, 'epoch': 18.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 1900/2525 [13:12<03:55,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1900\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.1203789710998535, 'eval_runtime': 2.5333, 'eval_samples_per_second': 179.607, 'eval_steps_per_second': 5.921, 'epoch': 18.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 79%|███████▉  | 2000/2525 [13:51<03:26,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2337, 'learning_rate': 4.158415841584159e-06, 'epoch': 19.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 79%|███████▉  | 2000/2525 [13:54<03:26,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2000\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.1326398849487305, 'eval_runtime': 2.5393, 'eval_samples_per_second': 179.183, 'eval_steps_per_second': 5.907, 'epoch': 19.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 83%|████████▎ | 2100/2525 [14:32<02:40,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2242, 'learning_rate': 3.3663366336633666e-06, 'epoch': 20.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 83%|████████▎ | 2100/2525 [14:35<02:40,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2100\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.146613597869873, 'eval_runtime': 2.5363, 'eval_samples_per_second': 179.395, 'eval_steps_per_second': 5.914, 'epoch': 20.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 87%|████████▋ | 2200/2525 [15:13<02:01,  2.67it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2035, 'learning_rate': 2.5742574257425744e-06, 'epoch': 21.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 87%|████████▋ | 2200/2525 [15:16<02:01,  2.67it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2200\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.101167678833008, 'eval_runtime': 2.5343, 'eval_samples_per_second': 179.536, 'eval_steps_per_second': 5.919, 'epoch': 21.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 91%|█████████ | 2300/2525 [15:55<01:28,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1947, 'learning_rate': 1.7821782178217822e-06, 'epoch': 22.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 91%|█████████ | 2300/2525 [15:58<01:28,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2300\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.109550952911377, 'eval_runtime': 2.5473, 'eval_samples_per_second': 178.619, 'eval_steps_per_second': 5.889, 'epoch': 22.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 95%|█████████▌| 2400/2525 [16:38<00:49,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1714, 'learning_rate': 9.900990099009902e-07, 'epoch': 23.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 95%|█████████▌| 2400/2525 [16:40<00:49,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2400\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.131380081176758, 'eval_runtime': 2.5383, 'eval_samples_per_second': 179.253, 'eval_steps_per_second': 5.909, 'epoch': 23.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      " 99%|█████████▉| 2500/2525 [17:19<00:09,  2.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1879, 'learning_rate': 1.9801980198019803e-07, 'epoch': 24.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 99%|█████████▉| 2500/2525 [17:21<00:09,  2.65it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2500\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_0\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.05354118347168, 'eval_runtime': 2.6344, 'eval_samples_per_second': 172.715, 'eval_steps_per_second': 5.694, 'epoch': 24.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "100%|██████████| 2525/2525 [17:32<00:00,  2.84it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 2525/2525 [17:32<00:00,  2.40it/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 974\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1052.2812, 'train_samples_per_second': 76.477, 'train_steps_per_second': 2.4, 'train_loss': 6.895829582592048, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:05<00:00,  5.75it/s]\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding adapter 'TAPT_sci-erc_1'.\n",
      "Adding head 'TAPT_sci-erc_1' with config {'head_type': 'masked_lm', 'vocab_size': 50265, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3219\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2525\n",
      "  4%|▍         | 100/2525 [00:39<15:47,  2.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.2072, 'learning_rate': 1.920792079207921e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  4%|▍         | 100/2525 [00:41<15:47,  2.56it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-100\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.700582504272461, 'eval_runtime': 2.5433, 'eval_samples_per_second': 178.901, 'eval_steps_per_second': 5.898, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "  8%|▊         | 200/2525 [01:20<14:35,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4945, 'learning_rate': 1.841584158415842e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  8%|▊         | 200/2525 [01:22<14:35,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-200\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.622188568115234, 'eval_runtime': 2.5463, 'eval_samples_per_second': 178.69, 'eval_steps_per_second': 5.891, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 12%|█▏        | 300/2525 [02:01<13:56,  2.66it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5736, 'learning_rate': 1.7623762376237624e-05, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▏        | 300/2525 [02:03<13:56,  2.66it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-300\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.231811046600342, 'eval_runtime': 2.5383, 'eval_samples_per_second': 179.253, 'eval_steps_per_second': 5.909, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 16%|█▌        | 400/2525 [02:43<13:50,  2.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2463, 'learning_rate': 1.683168316831683e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 16%|█▌        | 400/2525 [02:45<13:50,  2.56it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-400\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.939160346984863, 'eval_runtime': 2.5393, 'eval_samples_per_second': 179.183, 'eval_steps_per_second': 5.907, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 20%|█▉        | 500/2525 [03:25<13:12,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.0278, 'learning_rate': 1.6039603960396042e-05, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|█▉        | 500/2525 [03:28<13:12,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-500\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.793761253356934, 'eval_runtime': 2.5323, 'eval_samples_per_second': 179.678, 'eval_steps_per_second': 5.923, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 24%|██▍       | 600/2525 [04:07<12:29,  2.57it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.8764, 'learning_rate': 1.5247524752475249e-05, 'epoch': 5.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 24%|██▍       | 600/2525 [04:10<12:29,  2.57it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-600\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.65889835357666, 'eval_runtime': 2.5463, 'eval_samples_per_second': 178.69, 'eval_steps_per_second': 5.891, 'epoch': 5.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 28%|██▊       | 700/2525 [04:50<11:54,  2.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7407, 'learning_rate': 1.4455445544554456e-05, 'epoch': 6.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 28%|██▊       | 700/2525 [04:52<11:54,  2.56it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-700\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.551435947418213, 'eval_runtime': 2.5453, 'eval_samples_per_second': 178.76, 'eval_steps_per_second': 5.893, 'epoch': 6.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 32%|███▏      | 800/2525 [05:33<10:27,  2.75it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.6632, 'learning_rate': 1.3663366336633666e-05, 'epoch': 7.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 32%|███▏      | 800/2525 [05:35<10:27,  2.75it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-800\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.50976037979126, 'eval_runtime': 2.3371, 'eval_samples_per_second': 194.684, 'eval_steps_per_second': 6.418, 'epoch': 7.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 36%|███▌      | 900/2525 [06:11<09:16,  2.92it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5774, 'learning_rate': 1.2871287128712873e-05, 'epoch': 8.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 36%|███▌      | 900/2525 [06:13<09:16,  2.92it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-900\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.431258678436279, 'eval_runtime': 2.2971, 'eval_samples_per_second': 198.077, 'eval_steps_per_second': 6.53, 'epoch': 8.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 40%|███▉      | 1000/2525 [06:48<08:44,  2.91it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5208, 'learning_rate': 1.207920792079208e-05, 'epoch': 9.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|███▉      | 1000/2525 [06:50<08:44,  2.91it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1000\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.35998010635376, 'eval_runtime': 2.3732, 'eval_samples_per_second': 191.728, 'eval_steps_per_second': 6.321, 'epoch': 9.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 44%|████▎     | 1100/2525 [07:26<08:08,  2.92it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.4607, 'learning_rate': 1.1287128712871288e-05, 'epoch': 10.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 44%|████▎     | 1100/2525 [07:28<08:08,  2.92it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1100\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.308597087860107, 'eval_runtime': 2.3401, 'eval_samples_per_second': 194.434, 'eval_steps_per_second': 6.41, 'epoch': 10.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 48%|████▊     | 1200/2525 [08:03<07:31,  2.93it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3969, 'learning_rate': 1.0495049504950497e-05, 'epoch': 11.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 48%|████▊     | 1200/2525 [08:05<07:31,  2.93it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1200\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.288539886474609, 'eval_runtime': 2.2951, 'eval_samples_per_second': 198.25, 'eval_steps_per_second': 6.536, 'epoch': 11.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 51%|█████▏    | 1300/2525 [08:40<07:12,  2.83it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3693, 'learning_rate': 9.702970297029704e-06, 'epoch': 12.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 51%|█████▏    | 1300/2525 [08:43<07:12,  2.83it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1300\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.254106044769287, 'eval_runtime': 2.2941, 'eval_samples_per_second': 198.336, 'eval_steps_per_second': 6.539, 'epoch': 12.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 55%|█████▌    | 1400/2525 [09:18<06:25,  2.91it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3269, 'learning_rate': 8.910891089108911e-06, 'epoch': 13.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 55%|█████▌    | 1400/2525 [09:20<06:25,  2.91it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1400\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.2256975173950195, 'eval_runtime': 2.2922, 'eval_samples_per_second': 198.5, 'eval_steps_per_second': 6.544, 'epoch': 13.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 59%|█████▉    | 1500/2525 [09:57<06:10,  2.77it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3048, 'learning_rate': 8.11881188118812e-06, 'epoch': 14.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 59%|█████▉    | 1500/2525 [09:59<06:10,  2.77it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1500\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.170629978179932, 'eval_runtime': 2.4472, 'eval_samples_per_second': 185.925, 'eval_steps_per_second': 6.129, 'epoch': 14.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 63%|██████▎   | 1600/2525 [10:39<06:03,  2.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2919, 'learning_rate': 7.326732673267327e-06, 'epoch': 15.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 63%|██████▎   | 1600/2525 [10:42<06:03,  2.55it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1600\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.195589542388916, 'eval_runtime': 2.5793, 'eval_samples_per_second': 176.401, 'eval_steps_per_second': 5.815, 'epoch': 15.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 67%|██████▋   | 1700/2525 [11:20<05:10,  2.65it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2287, 'learning_rate': 6.534653465346535e-06, 'epoch': 16.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 1700/2525 [11:23<05:10,  2.65it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1700\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.157449245452881, 'eval_runtime': 2.5393, 'eval_samples_per_second': 179.183, 'eval_steps_per_second': 5.907, 'epoch': 16.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 71%|███████▏  | 1800/2525 [12:03<04:43,  2.56it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2, 'learning_rate': 5.7425742574257425e-06, 'epoch': 17.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 71%|███████▏  | 1800/2525 [12:06<04:43,  2.56it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1800\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.119650363922119, 'eval_runtime': 2.5443, 'eval_samples_per_second': 178.83, 'eval_steps_per_second': 5.896, 'epoch': 17.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 75%|███████▌  | 1900/2525 [12:50<04:03,  2.57it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.2102, 'learning_rate': 4.950495049504951e-06, 'epoch': 18.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 1900/2525 [12:53<04:03,  2.57it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-1900\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.055452823638916, 'eval_runtime': 2.6414, 'eval_samples_per_second': 172.257, 'eval_steps_per_second': 5.679, 'epoch': 18.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-1900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 79%|███████▉  | 2000/2525 [13:31<03:16,  2.67it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1866, 'learning_rate': 4.158415841584159e-06, 'epoch': 19.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 79%|███████▉  | 2000/2525 [13:33<03:16,  2.67it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2000\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.089317798614502, 'eval_runtime': 2.5603, 'eval_samples_per_second': 177.712, 'eval_steps_per_second': 5.859, 'epoch': 19.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 83%|████████▎ | 2100/2525 [14:10<02:24,  2.95it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1734, 'learning_rate': 3.3663366336633666e-06, 'epoch': 20.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 83%|████████▎ | 2100/2525 [14:12<02:24,  2.95it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2100\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.098305702209473, 'eval_runtime': 2.3131, 'eval_samples_per_second': 196.705, 'eval_steps_per_second': 6.485, 'epoch': 20.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 87%|████████▋ | 2200/2525 [14:47<01:50,  2.93it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.156, 'learning_rate': 2.5742574257425744e-06, 'epoch': 21.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 87%|████████▋ | 2200/2525 [14:49<01:50,  2.93it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2200\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.037253379821777, 'eval_runtime': 2.4302, 'eval_samples_per_second': 187.227, 'eval_steps_per_second': 6.172, 'epoch': 21.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 91%|█████████ | 2300/2525 [16:56<04:59,  1.33s/it]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1547, 'learning_rate': 1.7821782178217822e-06, 'epoch': 22.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 91%|█████████ | 2300/2525 [17:05<04:59,  1.33s/it]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2300\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.048826694488525, 'eval_runtime': 9.2134, 'eval_samples_per_second': 49.385, 'eval_steps_per_second': 1.628, 'epoch': 22.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 95%|█████████▌| 2400/2525 [19:19<02:46,  1.33s/it]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1252, 'learning_rate': 9.900990099009902e-07, 'epoch': 23.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 95%|█████████▌| 2400/2525 [19:28<02:46,  1.33s/it]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2400\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.0857415199279785, 'eval_runtime': 9.2118, 'eval_samples_per_second': 49.393, 'eval_steps_per_second': 1.628, 'epoch': 23.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      " 99%|█████████▉| 2500/2525 [21:41<00:33,  1.33s/it]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.1402, 'learning_rate': 1.9801980198019803e-07, 'epoch': 24.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 99%|█████████▉| 2500/2525 [21:51<00:33,  1.33s/it]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-2500\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_1\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.983608722686768, 'eval_runtime': 9.2124, 'eval_samples_per_second': 49.39, 'eval_steps_per_second': 1.628, 'epoch': 24.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-2500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "100%|██████████| 2525/2525 [22:24<00:00,  1.20s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 2525/2525 [22:24<00:00,  1.88it/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 974\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1344.8306, 'train_samples_per_second': 59.84, 'train_steps_per_second': 1.878, 'train_loss': 6.819621225489248, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:19<00:00,  1.57it/s]\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding adapter 'TAPT_sci-erc_2'.\n",
      "Adding head 'TAPT_sci-erc_2' with config {'head_type': 'masked_lm', 'vocab_size': 50265, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3219\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2525\n",
      "  4%|▍         | 100/2525 [02:15<53:48,  1.33s/it]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13.2072, 'learning_rate': 1.920792079207921e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  4%|▍         | 100/2525 [02:25<53:48,  1.33s/it]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-100\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.700582504272461, 'eval_runtime': 9.2114, 'eval_samples_per_second': 49.395, 'eval_steps_per_second': 1.628, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "  8%|▊         | 200/2525 [04:38<51:36,  1.33s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.4945, 'learning_rate': 1.841584158415842e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      "  8%|▊         | 200/2525 [04:47<51:36,  1.33s/it]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-200\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.622188568115234, 'eval_runtime': 9.2067, 'eval_samples_per_second': 49.421, 'eval_steps_per_second': 1.629, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      " 12%|█▏        | 300/2525 [07:01<49:17,  1.33s/it]  ***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5736, 'learning_rate': 1.7623762376237624e-05, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|█▏        | 300/2525 [07:10<49:17,  1.33s/it]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-300\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.231811046600342, 'eval_runtime': 9.2094, 'eval_samples_per_second': 49.406, 'eval_steps_per_second': 1.629, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      " 16%|█▌        | 400/2525 [08:29<14:00,  2.53it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.2463, 'learning_rate': 1.683168316831683e-05, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 16%|█▌        | 400/2525 [08:31<14:00,  2.53it/s]Saving model checkpoint to ./training_output/pretraining/TAPT\\checkpoint-400\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\head_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.939160346984863, 'eval_runtime': 2.5663, 'eval_samples_per_second': 177.296, 'eval_steps_per_second': 5.845, 'epoch': 3.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/pretraining/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      " 19%|█▉        | 486/2525 [09:05<13:13,  2.57it/s]"
     ]
    }
   ],
   "source": [
    "pretraining_loop(num_models = 3, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_pretraining, \n",
    "                 data_collator = data_collator, \n",
    "                 adapter_name = \"TAPT_sci-erc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('f1')\n",
    "\n",
    "def compute_metric(EvalPrediction):\n",
    "  \n",
    "  logits, labels = EvalPrediction\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return metric.compute(predictions=predictions, references=labels, average= 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuning_loop(num_models, training_args, dataset, adapter_name, num_labels, load_adapter = False, adapter_dir = 'Path'):\n",
    "\n",
    "    for i in range(num_models):\n",
    "        adapter = f\"{adapter_name}_{i}\"\n",
    "        model = model_init(adapter_name = adapter, num_lables = num_labels, pretraining=False, load_adapter = load_adapter, adapter_dir = f\"{adapter_dir}/{adapter}\")\n",
    "        \n",
    "        writer = SummaryWriter(log_dir= f'runs/{adapter}')\n",
    "        writer = TensorBoardCallback(writer)\n",
    "\n",
    "        trainer = AdapterTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            callbacks=[writer],\n",
    "            compute_metrics = compute_metric \n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        f = open(f\"{training_args.output_dir}/evaulations.txt\", \"a\")\n",
    "        f.write(adapter)\n",
    "        f.write(json.dumps(trainer.evaluate(dataset['test'])))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        # model.save_pretrained(f\"{adapter_name}\")\n",
    "        model.save_all_adapters(training_args.output_dir)\n",
    "        \n",
    "        trainer.remove_callback(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output/finetuning/DAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"DAPT_sci-erc\",\n",
    "                 load_adapter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DAPT+TAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./training_output/finetuning/DAPT_TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"DAPT_TAPT_sci-erc\",\n",
    "                 load_adapter = True,\n",
    "                 adapter_dir = \"./training_output/pretraining/DAPT_TAPT\",\n",
    "                 num_labels = num_of_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPT Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    logging_steps=25,\n",
    "    output_dir=\"./training_output/finetuning/TAPT\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3219\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1300\n",
      "  2%|▏         | 26/1300 [00:18<10:17,  2.06it/s] ***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  2%|▏         | 26/1300 [00:20<10:17,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6249961853027344, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.2441, 'eval_samples_per_second': 365.717, 'eval_steps_per_second': 3.215, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 52/1300 [00:36<10:00,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  4%|▍         | 52/1300 [00:37<10:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6099836826324463, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.2271, 'eval_samples_per_second': 370.788, 'eval_steps_per_second': 3.26, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 78/1300 [00:52<09:36,  2.12it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  6%|▌         | 78/1300 [00:54<09:36,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5678645372390747, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.2381, 'eval_samples_per_second': 367.491, 'eval_steps_per_second': 3.231, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 100/1300 [01:07<12:15,  1.63it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-100\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5121, 'learning_rate': 9.230769230769232e-05, 'epoch': 3.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 104/1300 [01:09<09:29,  2.10it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      "  8%|▊         | 104/1300 [01:10<09:29,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4916455745697021, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.2221, 'eval_samples_per_second': 372.306, 'eval_steps_per_second': 3.273, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 130/1300 [01:26<09:27,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 10%|█         | 130/1300 [01:28<09:27,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3861300945281982, 'eval_f1': 0.18125289031967284, 'eval_runtime': 1.2621, 'eval_samples_per_second': 360.497, 'eval_steps_per_second': 3.169, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 156/1300 [01:44<09:14,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 12%|█▏        | 156/1300 [01:45<09:14,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2312417030334473, 'eval_f1': 0.26120933287173925, 'eval_runtime': 1.2441, 'eval_samples_per_second': 365.717, 'eval_steps_per_second': 3.215, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 182/1300 [02:01<08:58,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 14%|█▍        | 182/1300 [02:02<08:58,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.19284987449646, 'eval_f1': 0.26786861930919553, 'eval_runtime': 1.2301, 'eval_samples_per_second': 369.883, 'eval_steps_per_second': 3.252, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 200/1300 [02:14<11:37,  1.58it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-200\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2102, 'learning_rate': 8.461538461538461e-05, 'epoch': 7.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 208/1300 [02:18<08:46,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 16%|█▌        | 208/1300 [02:20<08:46,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0569853782653809, 'eval_f1': 0.2883983430475202, 'eval_runtime': 1.2251, 'eval_samples_per_second': 371.394, 'eval_steps_per_second': 3.265, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 234/1300 [02:36<08:38,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 18%|█▊        | 234/1300 [02:37<08:38,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9305953979492188, 'eval_f1': 0.44503749785762475, 'eval_runtime': 1.2591, 'eval_samples_per_second': 361.356, 'eval_steps_per_second': 3.177, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 260/1300 [02:53<08:24,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 20%|██        | 260/1300 [02:55<08:24,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9127044081687927, 'eval_f1': 0.420200904434319, 'eval_runtime': 1.2501, 'eval_samples_per_second': 363.96, 'eval_steps_per_second': 3.2, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 286/1300 [03:11<08:21,  2.02it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 22%|██▏       | 286/1300 [03:12<08:21,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8339074850082397, 'eval_f1': 0.5371264748334814, 'eval_runtime': 1.2782, 'eval_samples_per_second': 355.98, 'eval_steps_per_second': 3.129, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 300/1300 [03:21<10:16,  1.62it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-300\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8643, 'learning_rate': 7.692307692307693e-05, 'epoch': 11.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 312/1300 [03:28<07:37,  2.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 24%|██▍       | 312/1300 [03:29<07:37,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7271323204040527, 'eval_f1': 0.5973265146483533, 'eval_runtime': 1.1851, 'eval_samples_per_second': 383.941, 'eval_steps_per_second': 3.375, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 338/1300 [03:44<07:23,  2.17it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 26%|██▌       | 338/1300 [03:45<07:23,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6377884149551392, 'eval_f1': 0.6996664533324589, 'eval_runtime': 1.2031, 'eval_samples_per_second': 378.192, 'eval_steps_per_second': 3.325, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 364/1300 [04:01<07:12,  2.17it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 28%|██▊       | 364/1300 [04:02<07:12,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.606914758682251, 'eval_f1': 0.6974308637360174, 'eval_runtime': 1.1831, 'eval_samples_per_second': 384.591, 'eval_steps_per_second': 3.381, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 390/1300 [04:17<07:06,  2.13it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 30%|███       | 390/1300 [04:19<07:06,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6291540861129761, 'eval_f1': 0.6883398834953353, 'eval_runtime': 1.2571, 'eval_samples_per_second': 361.932, 'eval_steps_per_second': 3.182, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 400/1300 [04:25<09:22,  1.60it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-400\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.623, 'learning_rate': 6.923076923076924e-05, 'epoch': 15.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 416/1300 [04:34<06:48,  2.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 32%|███▏      | 416/1300 [04:36<06:48,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5809431076049805, 'eval_f1': 0.7149590787419855, 'eval_runtime': 1.2011, 'eval_samples_per_second': 378.822, 'eval_steps_per_second': 3.33, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 442/1300 [04:51<06:52,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 34%|███▍      | 442/1300 [04:52<06:52,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5287930369377136, 'eval_f1': 0.7428402711517099, 'eval_runtime': 1.2371, 'eval_samples_per_second': 367.788, 'eval_steps_per_second': 3.233, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 468/1300 [05:08<06:41,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 36%|███▌      | 468/1300 [05:10<06:41,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5215913653373718, 'eval_f1': 0.7781508459476838, 'eval_runtime': 1.2411, 'eval_samples_per_second': 366.602, 'eval_steps_per_second': 3.223, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 494/1300 [05:26<06:30,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 38%|███▊      | 494/1300 [05:27<06:30,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5503784418106079, 'eval_f1': 0.7594456794138466, 'eval_runtime': 1.2431, 'eval_samples_per_second': 366.011, 'eval_steps_per_second': 3.218, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 500/1300 [05:31<09:05,  1.47it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-500\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.488, 'learning_rate': 6.153846153846155e-05, 'epoch': 19.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 520/1300 [05:43<06:17,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 40%|████      | 520/1300 [05:44<06:17,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5118339657783508, 'eval_f1': 0.7477409780575928, 'eval_runtime': 1.2401, 'eval_samples_per_second': 366.898, 'eval_steps_per_second': 3.225, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 546/1300 [06:00<06:03,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 42%|████▏     | 546/1300 [06:02<06:03,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.495577335357666, 'eval_f1': 0.7726403278469723, 'eval_runtime': 1.2331, 'eval_samples_per_second': 368.983, 'eval_steps_per_second': 3.244, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 572/1300 [06:18<05:51,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 44%|████▍     | 572/1300 [06:19<05:51,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48685941100120544, 'eval_f1': 0.7798224307109829, 'eval_runtime': 1.2251, 'eval_samples_per_second': 371.394, 'eval_steps_per_second': 3.265, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 598/1300 [06:35<05:37,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 46%|████▌     | 598/1300 [06:36<05:37,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4691464602947235, 'eval_f1': 0.7841418759657314, 'eval_runtime': 1.2331, 'eval_samples_per_second': 368.982, 'eval_steps_per_second': 3.244, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 600/1300 [06:38<09:36,  1.22it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-600\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3995, 'learning_rate': 5.384615384615385e-05, 'epoch': 23.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 624/1300 [06:52<05:15,  2.14it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 48%|████▊     | 624/1300 [06:54<05:15,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48051777482032776, 'eval_f1': 0.7888356095983335, 'eval_runtime': 1.2211, 'eval_samples_per_second': 372.612, 'eval_steps_per_second': 3.276, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 650/1300 [07:09<05:15,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 50%|█████     | 650/1300 [07:11<05:15,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47030210494995117, 'eval_f1': 0.7779960006076455, 'eval_runtime': 1.2281, 'eval_samples_per_second': 370.486, 'eval_steps_per_second': 3.257, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 676/1300 [07:26<04:59,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 52%|█████▏    | 676/1300 [07:28<04:59,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5272850394248962, 'eval_f1': 0.7881965448959081, 'eval_runtime': 1.2351, 'eval_samples_per_second': 368.384, 'eval_steps_per_second': 3.239, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 700/1300 [07:43<06:20,  1.58it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-700\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.361, 'learning_rate': 4.615384615384616e-05, 'epoch': 26.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 702/1300 [07:44<04:57,  2.01it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 54%|█████▍    | 702/1300 [07:45<04:57,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49519437551498413, 'eval_f1': 0.7778998121774452, 'eval_runtime': 1.2521, 'eval_samples_per_second': 363.379, 'eval_steps_per_second': 3.195, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 728/1300 [08:01<04:34,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 56%|█████▌    | 728/1300 [08:02<04:34,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4803767502307892, 'eval_f1': 0.7964605036154258, 'eval_runtime': 1.2641, 'eval_samples_per_second': 359.926, 'eval_steps_per_second': 3.164, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 754/1300 [08:18<04:22,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 58%|█████▊    | 754/1300 [08:20<04:22,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4797285199165344, 'eval_f1': 0.7830304150373656, 'eval_runtime': 1.2511, 'eval_samples_per_second': 363.669, 'eval_steps_per_second': 3.197, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 780/1300 [08:36<04:10,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 60%|██████    | 780/1300 [08:37<04:10,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4603773057460785, 'eval_f1': 0.7933248038511197, 'eval_runtime': 1.2291, 'eval_samples_per_second': 370.184, 'eval_steps_per_second': 3.254, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 800/1300 [08:50<05:21,  1.56it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-800\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3244, 'learning_rate': 3.846153846153846e-05, 'epoch': 30.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 806/1300 [08:53<04:00,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 62%|██████▏   | 806/1300 [08:55<04:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45797187089920044, 'eval_f1': 0.7991056955674606, 'eval_runtime': 1.2531, 'eval_samples_per_second': 363.088, 'eval_steps_per_second': 3.192, 'epoch': 31.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 832/1300 [09:11<03:48,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 64%|██████▍   | 832/1300 [09:12<03:48,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4680904746055603, 'eval_f1': 0.7973982158834615, 'eval_runtime': 1.2641, 'eval_samples_per_second': 359.926, 'eval_steps_per_second': 3.164, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 858/1300 [09:28<03:33,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 66%|██████▌   | 858/1300 [09:29<03:33,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46131619811058044, 'eval_f1': 0.7908350041082504, 'eval_runtime': 1.2566, 'eval_samples_per_second': 362.075, 'eval_steps_per_second': 3.183, 'epoch': 33.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 884/1300 [09:45<03:20,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 68%|██████▊   | 884/1300 [09:47<03:20,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46899569034576416, 'eval_f1': 0.7872055844612841, 'eval_runtime': 1.2722, 'eval_samples_per_second': 357.66, 'eval_steps_per_second': 3.144, 'epoch': 34.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 900/1300 [09:57<04:14,  1.57it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-900\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2911, 'learning_rate': 3.0769230769230774e-05, 'epoch': 34.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 910/1300 [10:03<03:07,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 70%|███████   | 910/1300 [10:04<03:07,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4673829674720764, 'eval_f1': 0.8001807171029804, 'eval_runtime': 1.2421, 'eval_samples_per_second': 366.306, 'eval_steps_per_second': 3.22, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 936/1300 [10:20<02:54,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 72%|███████▏  | 936/1300 [10:21<02:54,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4737717807292938, 'eval_f1': 0.7929461770818833, 'eval_runtime': 1.2451, 'eval_samples_per_second': 365.423, 'eval_steps_per_second': 3.213, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 962/1300 [10:37<02:42,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 74%|███████▍  | 962/1300 [10:39<02:42,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4730968475341797, 'eval_f1': 0.8066019444495174, 'eval_runtime': 1.2551, 'eval_samples_per_second': 362.509, 'eval_steps_per_second': 3.187, 'epoch': 37.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 988/1300 [10:55<02:30,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 76%|███████▌  | 988/1300 [10:56<02:30,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4754301905632019, 'eval_f1': 0.7930596961300659, 'eval_runtime': 1.2491, 'eval_samples_per_second': 364.252, 'eval_steps_per_second': 3.202, 'epoch': 38.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1000/1300 [11:03<03:11,  1.56it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-1000\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2648, 'learning_rate': 2.307692307692308e-05, 'epoch': 38.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1014/1300 [11:12<02:18,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 78%|███████▊  | 1014/1300 [11:13<02:18,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4696615934371948, 'eval_f1': 0.7772520480340678, 'eval_runtime': 1.2411, 'eval_samples_per_second': 366.602, 'eval_steps_per_second': 3.223, 'epoch': 39.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1040/1300 [11:30<02:05,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 80%|████████  | 1040/1300 [11:31<02:05,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4798038601875305, 'eval_f1': 0.7953696344370681, 'eval_runtime': 1.2611, 'eval_samples_per_second': 360.783, 'eval_steps_per_second': 3.172, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1066/1300 [11:47<01:52,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 82%|████████▏ | 1066/1300 [11:48<01:52,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4800928831100464, 'eval_f1': 0.7881968406448774, 'eval_runtime': 1.2581, 'eval_samples_per_second': 361.644, 'eval_steps_per_second': 3.179, 'epoch': 41.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1092/1300 [12:04<01:40,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 84%|████████▍ | 1092/1300 [12:05<01:40,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4717269837856293, 'eval_f1': 0.7950737256570599, 'eval_runtime': 1.2561, 'eval_samples_per_second': 362.22, 'eval_steps_per_second': 3.184, 'epoch': 42.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 1100/1300 [12:11<02:12,  1.51it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-1100\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2379, 'learning_rate': 1.5384615384615387e-05, 'epoch': 42.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1118/1300 [12:22<01:28,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 86%|████████▌ | 1118/1300 [12:23<01:28,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4661789536476135, 'eval_f1': 0.8084495352647831, 'eval_runtime': 1.2581, 'eval_samples_per_second': 361.644, 'eval_steps_per_second': 3.179, 'epoch': 43.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1144/1300 [12:39<01:15,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 88%|████████▊ | 1144/1300 [12:40<01:15,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4724625051021576, 'eval_f1': 0.7951878767620341, 'eval_runtime': 1.2486, 'eval_samples_per_second': 364.396, 'eval_steps_per_second': 3.203, 'epoch': 44.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1170/1300 [12:56<01:02,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 90%|█████████ | 1170/1300 [12:58<01:02,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47599875926971436, 'eval_f1': 0.7911448187814408, 'eval_runtime': 1.2591, 'eval_samples_per_second': 361.356, 'eval_steps_per_second': 3.177, 'epoch': 45.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1196/1300 [13:14<00:50,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 92%|█████████▏| 1196/1300 [13:15<00:50,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4824945330619812, 'eval_f1': 0.7990905865728409, 'eval_runtime': 1.2607, 'eval_samples_per_second': 360.925, 'eval_steps_per_second': 3.173, 'epoch': 46.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1200/1300 [13:18<01:13,  1.36it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-1200\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_0\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2366, 'learning_rate': 7.692307692307694e-06, 'epoch': 46.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1222/1300 [13:31<00:37,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 94%|█████████▍| 1222/1300 [13:33<00:37,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47304674983024597, 'eval_f1': 0.7785329425873374, 'eval_runtime': 1.2391, 'eval_samples_per_second': 367.194, 'eval_steps_per_second': 3.228, 'epoch': 47.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1248/1300 [13:49<00:25,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 96%|█████████▌| 1248/1300 [13:50<00:25,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4753786325454712, 'eval_f1': 0.7940395665650836, 'eval_runtime': 1.2611, 'eval_samples_per_second': 360.783, 'eval_steps_per_second': 3.172, 'epoch': 48.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1274/1300 [14:06<00:12,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 98%|█████████▊| 1274/1300 [14:07<00:12,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4750014841556549, 'eval_f1': 0.7853336806490319, 'eval_runtime': 1.2341, 'eval_samples_per_second': 368.683, 'eval_steps_per_second': 3.241, 'epoch': 49.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300/1300 [14:23<00:00,  2.04it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-1300\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2237, 'learning_rate': 0.0, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1300/1300 [14:25<00:00,  2.04it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1300/1300 [14:25<00:00,  1.50it/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 974\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47422534227371216, 'eval_f1': 0.7933629561668931, 'eval_runtime': 1.2551, 'eval_samples_per_second': 362.509, 'eval_steps_per_second': 3.187, 'epoch': 50.0}\n",
      "{'train_runtime': 865.2134, 'train_samples_per_second': 186.023, 'train_steps_per_second': 1.503, 'train_loss': 0.5412744213984563, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.48it/s]\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_0\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_0\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_0\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_0\\pytorch_model_head.bin\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading module configuration from ./training_output/pretraining/TAPT/TAPT_sci-erc_1\\adapter_config.json\n",
      "Adding adapter 'TAPT_sci-erc_1'.\n",
      "Loading module weights from ./training_output/pretraining/TAPT/TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Loading module configuration from ./training_output/pretraining/TAPT/TAPT_sci-erc_1\\head_config.json\n",
      "Adding head 'TAPT_sci-erc_1' with config {'head_type': 'masked_lm', 'vocab_size': 50265, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
      "Loading module weights from ./training_output/pretraining/TAPT/TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Adding head 'TAPT_sci-erc_1' with config {'head_type': 'classification', 'num_labels': 7, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'COMPARE': 0, 'CONJUNCTION': 1, 'EVALUATE-FOR': 2, 'FEATURE-OF': 3, 'HYPONYM-OF': 4, 'PART-OF': 5, 'USED-FOR': 6}, 'use_pooler': False, 'bias': True}.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3219\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1300\n",
      "  2%|▏         | 26/1300 [00:15<10:02,  2.12it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  2%|▏         | 26/1300 [00:16<10:02,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6253387928009033, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.1961, 'eval_samples_per_second': 380.407, 'eval_steps_per_second': 3.344, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 52/1300 [00:32<09:41,  2.15it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  4%|▍         | 52/1300 [00:33<09:41,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6060683727264404, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.1961, 'eval_samples_per_second': 380.407, 'eval_steps_per_second': 3.344, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 78/1300 [00:49<09:54,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  6%|▌         | 78/1300 [00:50<09:54,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5492271184921265, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.2371, 'eval_samples_per_second': 367.788, 'eval_steps_per_second': 3.233, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 100/1300 [01:04<12:43,  1.57it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-100\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.504, 'learning_rate': 9.230769230769232e-05, 'epoch': 3.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 104/1300 [01:06<09:48,  2.03it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      "  8%|▊         | 104/1300 [01:08<09:48,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3890310525894165, 'eval_f1': 0.12934148030872747, 'eval_runtime': 1.2381, 'eval_samples_per_second': 367.491, 'eval_steps_per_second': 3.231, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 130/1300 [01:24<09:24,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 10%|█         | 130/1300 [01:25<09:24,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2166248559951782, 'eval_f1': 0.23385161745623417, 'eval_runtime': 1.2361, 'eval_samples_per_second': 368.086, 'eval_steps_per_second': 3.236, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 156/1300 [01:41<09:14,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 12%|█▏        | 156/1300 [01:42<09:14,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0576788187026978, 'eval_f1': 0.3120256920261422, 'eval_runtime': 1.2511, 'eval_samples_per_second': 363.669, 'eval_steps_per_second': 3.197, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 182/1300 [01:58<09:04,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 14%|█▍        | 182/1300 [02:00<09:04,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.022516131401062, 'eval_f1': 0.4376902131043582, 'eval_runtime': 1.2581, 'eval_samples_per_second': 361.644, 'eval_steps_per_second': 3.179, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 200/1300 [02:11<11:44,  1.56it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-200\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0776, 'learning_rate': 8.461538461538461e-05, 'epoch': 7.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 208/1300 [02:16<08:47,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 16%|█▌        | 208/1300 [02:17<08:47,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8700148463249207, 'eval_f1': 0.43966913984990325, 'eval_runtime': 1.2261, 'eval_samples_per_second': 371.091, 'eval_steps_per_second': 3.262, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 234/1300 [02:33<08:40,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 18%|█▊        | 234/1300 [02:35<08:40,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7941710948944092, 'eval_f1': 0.5096297534366085, 'eval_runtime': 1.2561, 'eval_samples_per_second': 362.22, 'eval_steps_per_second': 3.184, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 260/1300 [02:51<08:25,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 20%|██        | 260/1300 [02:52<08:25,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7286162376403809, 'eval_f1': 0.5852453581896607, 'eval_runtime': 1.2421, 'eval_samples_per_second': 366.306, 'eval_steps_per_second': 3.22, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 286/1300 [03:08<08:07,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 22%|██▏       | 286/1300 [03:10<08:07,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7432776689529419, 'eval_f1': 0.564028384961748, 'eval_runtime': 1.2311, 'eval_samples_per_second': 369.582, 'eval_steps_per_second': 3.249, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 300/1300 [03:18<10:40,  1.56it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-300\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7423, 'learning_rate': 7.692307692307693e-05, 'epoch': 11.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 312/1300 [03:26<07:58,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 24%|██▍       | 312/1300 [03:27<07:58,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6189000606536865, 'eval_f1': 0.6976502663014557, 'eval_runtime': 1.2411, 'eval_samples_per_second': 366.602, 'eval_steps_per_second': 3.223, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 338/1300 [03:43<07:47,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 26%|██▌       | 338/1300 [03:44<07:47,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5857757925987244, 'eval_f1': 0.7200501150606388, 'eval_runtime': 1.2647, 'eval_samples_per_second': 359.782, 'eval_steps_per_second': 3.163, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 364/1300 [04:00<07:30,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 28%|██▊       | 364/1300 [04:02<07:30,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5606494545936584, 'eval_f1': 0.7317291541302985, 'eval_runtime': 1.2311, 'eval_samples_per_second': 369.582, 'eval_steps_per_second': 3.249, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 390/1300 [04:18<07:17,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 30%|███       | 390/1300 [04:19<07:17,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5267325639724731, 'eval_f1': 0.6997116335087565, 'eval_runtime': 1.2291, 'eval_samples_per_second': 370.184, 'eval_steps_per_second': 3.254, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 400/1300 [04:25<09:43,  1.54it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-400\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5659, 'learning_rate': 6.923076923076924e-05, 'epoch': 15.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 416/1300 [04:35<07:07,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 32%|███▏      | 416/1300 [04:36<07:07,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5276452898979187, 'eval_f1': 0.7173447975353678, 'eval_runtime': 1.2351, 'eval_samples_per_second': 368.384, 'eval_steps_per_second': 3.239, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 442/1300 [04:52<06:53,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 34%|███▍      | 442/1300 [04:54<06:53,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48866844177246094, 'eval_f1': 0.7723283121822199, 'eval_runtime': 1.2571, 'eval_samples_per_second': 361.932, 'eval_steps_per_second': 3.182, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 468/1300 [05:10<06:43,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 36%|███▌      | 468/1300 [05:11<06:43,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46511778235435486, 'eval_f1': 0.7778505496110393, 'eval_runtime': 1.2631, 'eval_samples_per_second': 360.211, 'eval_steps_per_second': 3.167, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 494/1300 [05:27<06:29,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 38%|███▊      | 494/1300 [05:28<06:29,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46452796459198, 'eval_f1': 0.7986332941845611, 'eval_runtime': 1.2611, 'eval_samples_per_second': 360.783, 'eval_steps_per_second': 3.172, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 500/1300 [05:32<09:09,  1.46it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-500\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4542, 'learning_rate': 6.153846153846155e-05, 'epoch': 19.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 520/1300 [05:45<06:14,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 40%|████      | 520/1300 [05:46<06:14,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48290374875068665, 'eval_f1': 0.7796897038833325, 'eval_runtime': 1.2682, 'eval_samples_per_second': 358.79, 'eval_steps_per_second': 3.154, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 546/1300 [06:02<06:01,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 42%|████▏     | 546/1300 [06:03<06:01,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4693048298358917, 'eval_f1': 0.7745990841329352, 'eval_runtime': 1.2471, 'eval_samples_per_second': 364.836, 'eval_steps_per_second': 3.207, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 572/1300 [06:19<05:49,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 44%|████▍     | 572/1300 [06:20<05:49,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46345871686935425, 'eval_f1': 0.7900722454155972, 'eval_runtime': 1.2251, 'eval_samples_per_second': 371.394, 'eval_steps_per_second': 3.265, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 598/1300 [06:36<05:36,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 46%|████▌     | 598/1300 [06:38<05:36,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44969382882118225, 'eval_f1': 0.7947951271293391, 'eval_runtime': 1.2281, 'eval_samples_per_second': 370.486, 'eval_steps_per_second': 3.257, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 600/1300 [06:39<09:34,  1.22it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-600\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3759, 'learning_rate': 5.384615384615385e-05, 'epoch': 23.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 624/1300 [06:54<05:24,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 48%|████▊     | 624/1300 [06:55<05:24,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.431561678647995, 'eval_f1': 0.801582599075178, 'eval_runtime': 1.2301, 'eval_samples_per_second': 369.883, 'eval_steps_per_second': 3.252, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 650/1300 [07:11<05:11,  2.09it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 50%|█████     | 650/1300 [07:12<05:11,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4343816936016083, 'eval_f1': 0.7983546435591532, 'eval_runtime': 1.2281, 'eval_samples_per_second': 370.486, 'eval_steps_per_second': 3.257, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 676/1300 [07:28<04:59,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 52%|█████▏    | 676/1300 [07:29<04:59,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4644463360309601, 'eval_f1': 0.7766793249534467, 'eval_runtime': 1.2291, 'eval_samples_per_second': 370.184, 'eval_steps_per_second': 3.254, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 700/1300 [07:45<06:20,  1.58it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-700\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3315, 'learning_rate': 4.615384615384616e-05, 'epoch': 26.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 702/1300 [07:45<04:56,  2.02it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 54%|█████▍    | 702/1300 [07:47<04:56,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.442306250333786, 'eval_f1': 0.790723168195284, 'eval_runtime': 1.2401, 'eval_samples_per_second': 366.898, 'eval_steps_per_second': 3.225, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 728/1300 [08:03<04:34,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 56%|█████▌    | 728/1300 [08:04<04:34,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4378008246421814, 'eval_f1': 0.7874977125911827, 'eval_runtime': 1.2341, 'eval_samples_per_second': 368.683, 'eval_steps_per_second': 3.241, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 754/1300 [08:20<04:21,  2.09it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 58%|█████▊    | 754/1300 [08:21<04:21,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4584326148033142, 'eval_f1': 0.7937900907851045, 'eval_runtime': 1.2281, 'eval_samples_per_second': 370.486, 'eval_steps_per_second': 3.257, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 780/1300 [08:37<04:09,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 60%|██████    | 780/1300 [08:38<04:09,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41434669494628906, 'eval_f1': 0.8157173578633508, 'eval_runtime': 1.2301, 'eval_samples_per_second': 369.883, 'eval_steps_per_second': 3.252, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 800/1300 [08:51<05:17,  1.58it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-800\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2916, 'learning_rate': 3.846153846153846e-05, 'epoch': 30.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 806/1300 [08:54<03:59,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 62%|██████▏   | 806/1300 [08:56<03:59,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43067559599876404, 'eval_f1': 0.8088159960888186, 'eval_runtime': 1.2281, 'eval_samples_per_second': 370.486, 'eval_steps_per_second': 3.257, 'epoch': 31.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 832/1300 [09:12<03:44,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 64%|██████▍   | 832/1300 [09:13<03:44,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4278183877468109, 'eval_f1': 0.7943271908460844, 'eval_runtime': 1.2341, 'eval_samples_per_second': 368.683, 'eval_steps_per_second': 3.241, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 858/1300 [09:29<03:32,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 66%|██████▌   | 858/1300 [09:30<03:32,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4233522415161133, 'eval_f1': 0.7986194827333398, 'eval_runtime': 1.2301, 'eval_samples_per_second': 369.883, 'eval_steps_per_second': 3.252, 'epoch': 33.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 884/1300 [09:46<03:19,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 68%|██████▊   | 884/1300 [09:47<03:19,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42074882984161377, 'eval_f1': 0.8065498418283615, 'eval_runtime': 1.2351, 'eval_samples_per_second': 368.384, 'eval_steps_per_second': 3.239, 'epoch': 34.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 900/1300 [09:57<04:14,  1.57it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-900\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2604, 'learning_rate': 3.0769230769230774e-05, 'epoch': 34.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 910/1300 [10:03<03:09,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 70%|███████   | 910/1300 [10:05<03:09,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41444507241249084, 'eval_f1': 0.8137709960313467, 'eval_runtime': 1.2371, 'eval_samples_per_second': 367.788, 'eval_steps_per_second': 3.233, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 936/1300 [10:21<02:55,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 72%|███████▏  | 936/1300 [10:22<02:55,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4275467097759247, 'eval_f1': 0.805423098281928, 'eval_runtime': 1.2251, 'eval_samples_per_second': 371.394, 'eval_steps_per_second': 3.265, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 962/1300 [10:38<02:42,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 74%|███████▍  | 962/1300 [10:39<02:42,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42908158898353577, 'eval_f1': 0.8022843022723843, 'eval_runtime': 1.2261, 'eval_samples_per_second': 371.091, 'eval_steps_per_second': 3.262, 'epoch': 37.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 988/1300 [10:55<02:29,  2.09it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 76%|███████▌  | 988/1300 [10:56<02:29,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4211490750312805, 'eval_f1': 0.8084906166229879, 'eval_runtime': 1.2311, 'eval_samples_per_second': 369.582, 'eval_steps_per_second': 3.249, 'epoch': 38.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1000/1300 [11:04<03:11,  1.56it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-1000\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1000\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2425, 'learning_rate': 2.307692307692308e-05, 'epoch': 38.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1014/1300 [11:13<02:17,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 78%|███████▊  | 1014/1300 [11:14<02:17,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4376086890697479, 'eval_f1': 0.7967585864755259, 'eval_runtime': 1.2271, 'eval_samples_per_second': 370.788, 'eval_steps_per_second': 3.26, 'epoch': 39.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1040/1300 [11:30<02:05,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 80%|████████  | 1040/1300 [11:31<02:05,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4253290593624115, 'eval_f1': 0.8030406592609476, 'eval_runtime': 1.2351, 'eval_samples_per_second': 368.384, 'eval_steps_per_second': 3.239, 'epoch': 40.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1066/1300 [11:47<01:52,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 82%|████████▏ | 1066/1300 [11:48<01:52,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4231235682964325, 'eval_f1': 0.8031418695862869, 'eval_runtime': 1.2331, 'eval_samples_per_second': 368.982, 'eval_steps_per_second': 3.244, 'epoch': 41.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 1092/1300 [12:04<01:39,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 84%|████████▍ | 1092/1300 [12:05<01:39,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4229285418987274, 'eval_f1': 0.8015426679776898, 'eval_runtime': 1.2351, 'eval_samples_per_second': 368.385, 'eval_steps_per_second': 3.239, 'epoch': 42.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 1100/1300 [12:11<02:11,  1.52it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-1100\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1100\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2191, 'learning_rate': 1.5384615384615387e-05, 'epoch': 42.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1118/1300 [12:22<01:27,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 86%|████████▌ | 1118/1300 [12:23<01:27,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41809317469596863, 'eval_f1': 0.8169267475745434, 'eval_runtime': 1.2261, 'eval_samples_per_second': 371.091, 'eval_steps_per_second': 3.262, 'epoch': 43.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1144/1300 [12:39<01:15,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 88%|████████▊ | 1144/1300 [12:40<01:15,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42621326446533203, 'eval_f1': 0.8077144427878501, 'eval_runtime': 1.2331, 'eval_samples_per_second': 368.982, 'eval_steps_per_second': 3.244, 'epoch': 44.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1170/1300 [12:56<01:02,  2.09it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 90%|█████████ | 1170/1300 [12:57<01:02,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42312243580818176, 'eval_f1': 0.8024127642121355, 'eval_runtime': 1.2341, 'eval_samples_per_second': 368.683, 'eval_steps_per_second': 3.241, 'epoch': 45.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1196/1300 [13:13<00:49,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 92%|█████████▏| 1196/1300 [13:15<00:49,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42973431944847107, 'eval_f1': 0.819414332388859, 'eval_runtime': 1.2311, 'eval_samples_per_second': 369.583, 'eval_steps_per_second': 3.249, 'epoch': 46.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1200/1300 [13:17<01:12,  1.38it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-1200\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1200\\TAPT_sci-erc_1\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.213, 'learning_rate': 7.692307692307694e-06, 'epoch': 46.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1222/1300 [13:31<00:37,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 94%|█████████▍| 1222/1300 [13:32<00:37,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41795986890792847, 'eval_f1': 0.8173346565431012, 'eval_runtime': 1.2531, 'eval_samples_per_second': 363.088, 'eval_steps_per_second': 3.192, 'epoch': 47.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1248/1300 [13:48<00:24,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 96%|█████████▌| 1248/1300 [13:49<00:24,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4210352599620819, 'eval_f1': 0.8172406064216898, 'eval_runtime': 1.2481, 'eval_samples_per_second': 364.544, 'eval_steps_per_second': 3.205, 'epoch': 48.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1274/1300 [14:05<00:12,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                   \n",
      " 98%|█████████▊| 1274/1300 [14:06<00:12,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41785070300102234, 'eval_f1': 0.8184952685995907, 'eval_runtime': 1.2421, 'eval_samples_per_second': 366.306, 'eval_steps_per_second': 3.22, 'epoch': 49.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1300/1300 [14:22<00:00,  2.08it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-1300\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-1300\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2027, 'learning_rate': 0.0, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1300/1300 [14:24<00:00,  2.08it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 1300/1300 [14:24<00:00,  1.50it/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 974\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.419697105884552, 'eval_f1': 0.8228407955680684, 'eval_runtime': 1.2501, 'eval_samples_per_second': 363.96, 'eval_steps_per_second': 3.2, 'epoch': 50.0}\n",
      "{'train_runtime': 864.3591, 'train_samples_per_second': 186.207, 'train_steps_per_second': 1.504, 'train_loss': 0.4985182336660532, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.48it/s]\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_1\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_1\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_1\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\TAPT_sci-erc_1\\pytorch_model_head.bin\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading module configuration from ./training_output/pretraining/TAPT/TAPT_sci-erc_2\\adapter_config.json\n",
      "Adding adapter 'TAPT_sci-erc_2'.\n",
      "Loading module weights from ./training_output/pretraining/TAPT/TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Loading module configuration from ./training_output/pretraining/TAPT/TAPT_sci-erc_2\\head_config.json\n",
      "Adding head 'TAPT_sci-erc_2' with config {'head_type': 'masked_lm', 'vocab_size': 50265, 'layers': 2, 'activation_function': 'gelu', 'layer_norm': True, 'bias': True, 'shift_labels': False, 'label2id': None}.\n",
      "Loading module weights from ./training_output/pretraining/TAPT/TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Adding head 'TAPT_sci-erc_2' with config {'head_type': 'classification', 'num_labels': 7, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'COMPARE': 0, 'CONJUNCTION': 1, 'EVALUATE-FOR': 2, 'FEATURE-OF': 3, 'HYPONYM-OF': 4, 'PART-OF': 5, 'USED-FOR': 6}, 'use_pooler': False, 'bias': True}.\n",
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "AdapterTrainerCallback\n",
      "C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3219\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1300\n",
      "  2%|▏         | 26/1300 [00:16<10:23,  2.04it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  2%|▏         | 26/1300 [00:17<10:23,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6253387928009033, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.2512, 'eval_samples_per_second': 363.652, 'eval_steps_per_second': 3.197, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 52/1300 [00:33<10:01,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  4%|▍         | 52/1300 [00:34<10:01,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6060683727264404, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.2311, 'eval_samples_per_second': 369.582, 'eval_steps_per_second': 3.249, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 78/1300 [00:50<09:56,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                 \n",
      "  6%|▌         | 78/1300 [00:52<09:56,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5492271184921265, 'eval_f1': 0.09139440529575059, 'eval_runtime': 1.2682, 'eval_samples_per_second': 358.789, 'eval_steps_per_second': 3.154, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 100/1300 [01:06<12:49,  1.56it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-100\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-100\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.504, 'learning_rate': 9.230769230769232e-05, 'epoch': 3.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 104/1300 [01:08<09:55,  2.01it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      "  8%|▊         | 104/1300 [01:09<09:55,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3890310525894165, 'eval_f1': 0.12934148030872747, 'eval_runtime': 1.2611, 'eval_samples_per_second': 360.783, 'eval_steps_per_second': 3.172, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 130/1300 [01:25<09:32,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 10%|█         | 130/1300 [01:27<09:32,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2166248559951782, 'eval_f1': 0.23385161745623417, 'eval_runtime': 1.2662, 'eval_samples_per_second': 359.357, 'eval_steps_per_second': 3.159, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 156/1300 [01:43<09:17,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 12%|█▏        | 156/1300 [01:44<09:17,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0576788187026978, 'eval_f1': 0.3120256920261422, 'eval_runtime': 1.2481, 'eval_samples_per_second': 364.544, 'eval_steps_per_second': 3.205, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 182/1300 [02:00<08:59,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 14%|█▍        | 182/1300 [02:01<08:59,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.022516131401062, 'eval_f1': 0.4376902131043582, 'eval_runtime': 1.2451, 'eval_samples_per_second': 365.423, 'eval_steps_per_second': 3.213, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 200/1300 [02:13<11:45,  1.56it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-200\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-200\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0776, 'learning_rate': 8.461538461538461e-05, 'epoch': 7.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 208/1300 [02:18<08:46,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 16%|█▌        | 208/1300 [02:19<08:46,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8700148463249207, 'eval_f1': 0.43966913984990325, 'eval_runtime': 1.2321, 'eval_samples_per_second': 369.282, 'eval_steps_per_second': 3.246, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 234/1300 [02:35<08:35,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 18%|█▊        | 234/1300 [02:36<08:35,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7941710948944092, 'eval_f1': 0.5096297534366085, 'eval_runtime': 1.2601, 'eval_samples_per_second': 361.069, 'eval_steps_per_second': 3.174, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 260/1300 [02:52<08:26,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 20%|██        | 260/1300 [02:54<08:26,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7286162376403809, 'eval_f1': 0.5852453581896607, 'eval_runtime': 1.2531, 'eval_samples_per_second': 363.088, 'eval_steps_per_second': 3.192, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 286/1300 [03:10<08:13,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 22%|██▏       | 286/1300 [03:11<08:13,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7432776689529419, 'eval_f1': 0.564028384961748, 'eval_runtime': 1.2797, 'eval_samples_per_second': 355.561, 'eval_steps_per_second': 3.126, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 300/1300 [03:20<10:37,  1.57it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-300\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-300\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7423, 'learning_rate': 7.692307692307693e-05, 'epoch': 11.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 312/1300 [03:27<07:58,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 24%|██▍       | 312/1300 [03:28<07:58,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6189000606536865, 'eval_f1': 0.6976502663014557, 'eval_runtime': 1.2491, 'eval_samples_per_second': 364.252, 'eval_steps_per_second': 3.202, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 338/1300 [03:45<07:41,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 26%|██▌       | 338/1300 [03:46<07:41,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5857757925987244, 'eval_f1': 0.7200501150606388, 'eval_runtime': 1.2401, 'eval_samples_per_second': 366.898, 'eval_steps_per_second': 3.225, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 364/1300 [04:02<07:29,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 28%|██▊       | 364/1300 [04:03<07:29,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5606494545936584, 'eval_f1': 0.7317291541302985, 'eval_runtime': 1.2571, 'eval_samples_per_second': 361.932, 'eval_steps_per_second': 3.182, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 390/1300 [04:19<07:20,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 30%|███       | 390/1300 [04:21<07:20,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5267325639724731, 'eval_f1': 0.6997116335087565, 'eval_runtime': 1.2131, 'eval_samples_per_second': 375.071, 'eval_steps_per_second': 3.297, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 400/1300 [04:27<09:40,  1.55it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-400\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-400\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5659, 'learning_rate': 6.923076923076924e-05, 'epoch': 15.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 416/1300 [04:37<07:11,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 32%|███▏      | 416/1300 [04:38<07:11,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5276452898979187, 'eval_f1': 0.7173447975353678, 'eval_runtime': 1.2471, 'eval_samples_per_second': 364.836, 'eval_steps_per_second': 3.207, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 442/1300 [04:54<06:54,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 34%|███▍      | 442/1300 [04:55<06:54,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48866844177246094, 'eval_f1': 0.7723283121822199, 'eval_runtime': 1.2271, 'eval_samples_per_second': 370.788, 'eval_steps_per_second': 3.26, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 468/1300 [05:11<06:42,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 36%|███▌      | 468/1300 [05:13<06:42,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46511778235435486, 'eval_f1': 0.7778505496110393, 'eval_runtime': 1.2311, 'eval_samples_per_second': 369.582, 'eval_steps_per_second': 3.249, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 494/1300 [05:29<06:30,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 38%|███▊      | 494/1300 [05:30<06:30,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46452796459198, 'eval_f1': 0.7986332941845611, 'eval_runtime': 1.2421, 'eval_samples_per_second': 366.307, 'eval_steps_per_second': 3.22, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 500/1300 [05:34<09:12,  1.45it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-500\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-500\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4542, 'learning_rate': 6.153846153846155e-05, 'epoch': 19.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 520/1300 [05:46<06:14,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 40%|████      | 520/1300 [05:48<06:14,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48290374875068665, 'eval_f1': 0.7796897038833325, 'eval_runtime': 1.2301, 'eval_samples_per_second': 369.883, 'eval_steps_per_second': 3.252, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 546/1300 [06:04<06:05,  2.06it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 42%|████▏     | 546/1300 [06:05<06:05,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4693048298358917, 'eval_f1': 0.7745990841329352, 'eval_runtime': 1.2571, 'eval_samples_per_second': 361.932, 'eval_steps_per_second': 3.182, 'epoch': 21.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 572/1300 [06:21<05:44,  2.11it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 44%|████▍     | 572/1300 [06:22<05:44,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46345871686935425, 'eval_f1': 0.7900722454155972, 'eval_runtime': 1.1841, 'eval_samples_per_second': 384.266, 'eval_steps_per_second': 3.378, 'epoch': 22.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 598/1300 [06:38<05:25,  2.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 46%|████▌     | 598/1300 [06:39<05:25,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44969382882118225, 'eval_f1': 0.7947951271293391, 'eval_runtime': 1.1851, 'eval_samples_per_second': 383.941, 'eval_steps_per_second': 3.375, 'epoch': 23.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 600/1300 [06:40<09:23,  1.24it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-600\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-600\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3759, 'learning_rate': 5.384615384615385e-05, 'epoch': 23.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 624/1300 [06:55<05:25,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 48%|████▊     | 624/1300 [06:56<05:25,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.431561678647995, 'eval_f1': 0.801582599075178, 'eval_runtime': 1.2411, 'eval_samples_per_second': 366.602, 'eval_steps_per_second': 3.223, 'epoch': 24.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 650/1300 [07:12<05:05,  2.12it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 50%|█████     | 650/1300 [07:14<05:05,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4343816936016083, 'eval_f1': 0.7983546435591532, 'eval_runtime': 1.2161, 'eval_samples_per_second': 374.145, 'eval_steps_per_second': 3.289, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 676/1300 [07:29<04:55,  2.11it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 52%|█████▏    | 676/1300 [07:30<04:55,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4644463360309601, 'eval_f1': 0.7766793249534467, 'eval_runtime': 1.2021, 'eval_samples_per_second': 378.507, 'eval_steps_per_second': 3.328, 'epoch': 26.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 700/1300 [07:45<06:08,  1.63it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-700\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-700\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3315, 'learning_rate': 4.615384615384616e-05, 'epoch': 26.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 702/1300 [07:46<04:51,  2.05it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 54%|█████▍    | 702/1300 [07:47<04:51,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.442306250333786, 'eval_f1': 0.790723168195284, 'eval_runtime': 1.2231, 'eval_samples_per_second': 372.002, 'eval_steps_per_second': 3.27, 'epoch': 27.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 728/1300 [08:03<04:24,  2.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 56%|█████▌    | 728/1300 [08:04<04:24,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4378008246421814, 'eval_f1': 0.7874977125911827, 'eval_runtime': 1.2281, 'eval_samples_per_second': 370.486, 'eval_steps_per_second': 3.257, 'epoch': 28.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 754/1300 [08:20<04:20,  2.10it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 58%|█████▊    | 754/1300 [08:21<04:20,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4584326148033142, 'eval_f1': 0.7937900907851045, 'eval_runtime': 1.2171, 'eval_samples_per_second': 373.837, 'eval_steps_per_second': 3.286, 'epoch': 29.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 780/1300 [08:37<04:04,  2.12it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 60%|██████    | 780/1300 [08:38<04:04,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41434669494628906, 'eval_f1': 0.8157173578633508, 'eval_runtime': 1.2031, 'eval_samples_per_second': 378.192, 'eval_steps_per_second': 3.325, 'epoch': 30.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 800/1300 [08:50<05:07,  1.62it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-800\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-800\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2916, 'learning_rate': 3.846153846153846e-05, 'epoch': 30.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 806/1300 [08:54<03:53,  2.12it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 62%|██████▏   | 806/1300 [08:55<03:53,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43067559599876404, 'eval_f1': 0.8088159960888186, 'eval_runtime': 1.1931, 'eval_samples_per_second': 381.364, 'eval_steps_per_second': 3.353, 'epoch': 31.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 832/1300 [09:10<03:36,  2.16it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 64%|██████▍   | 832/1300 [09:11<03:36,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4278183877468109, 'eval_f1': 0.7943271908460844, 'eval_runtime': 1.1871, 'eval_samples_per_second': 383.294, 'eval_steps_per_second': 3.37, 'epoch': 32.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 858/1300 [09:27<03:32,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 66%|██████▌   | 858/1300 [09:28<03:32,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4233522415161133, 'eval_f1': 0.7986194827333398, 'eval_runtime': 1.2321, 'eval_samples_per_second': 369.282, 'eval_steps_per_second': 3.246, 'epoch': 33.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 884/1300 [09:44<03:19,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 68%|██████▊   | 884/1300 [09:46<03:19,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42074882984161377, 'eval_f1': 0.8065498418283615, 'eval_runtime': 1.2311, 'eval_samples_per_second': 369.582, 'eval_steps_per_second': 3.249, 'epoch': 34.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 900/1300 [09:56<04:14,  1.57it/s]Saving model checkpoint to ./training_output/finetuning/TAPT\\checkpoint-900\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_2\\adapter_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_2\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_2\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_2\\head_config.json\n",
      "Module weights saved in ./training_output/finetuning/TAPT\\checkpoint-900\\TAPT_sci-erc_2\\pytorch_model_head.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2604, 'learning_rate': 3.0769230769230774e-05, 'epoch': 34.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 910/1300 [10:02<03:07,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 70%|███████   | 910/1300 [10:03<03:07,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41444507241249084, 'eval_f1': 0.8137709960313467, 'eval_runtime': 1.2301, 'eval_samples_per_second': 369.883, 'eval_steps_per_second': 3.252, 'epoch': 35.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 936/1300 [10:19<02:54,  2.08it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 72%|███████▏  | 936/1300 [10:20<02:54,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4275467097759247, 'eval_f1': 0.805423098281928, 'eval_runtime': 1.2421, 'eval_samples_per_second': 366.307, 'eval_steps_per_second': 3.22, 'epoch': 36.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 962/1300 [10:36<02:43,  2.07it/s]***** Running Evaluation *****\n",
      "  Num examples = 455\n",
      "  Batch size = 128\n",
      "                                                  \n",
      " 74%|███████▍  | 962/1300 [10:38<02:43,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42908158898353577, 'eval_f1': 0.8022843022723843, 'eval_runtime': 1.2702, 'eval_samples_per_second': 358.224, 'eval_steps_per_second': 3.149, 'epoch': 37.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 985/1300 [10:52<03:22,  1.56it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000033?line=0'>1</a>\u001b[0m finetuning_loop(num_models \u001b[39m=\u001b[39;49m \u001b[39m5\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000033?line=1'>2</a>\u001b[0m                  training_args \u001b[39m=\u001b[39;49m training_args, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000033?line=2'>3</a>\u001b[0m                  dataset \u001b[39m=\u001b[39;49m scierc_dataset_finetuning,  \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000033?line=3'>4</a>\u001b[0m                  adapter_name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mTAPT_sci-erc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000033?line=4'>5</a>\u001b[0m                  load_adapter \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000033?line=5'>6</a>\u001b[0m                  adapter_dir \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m./training_output/pretraining/TAPT\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000033?line=6'>7</a>\u001b[0m                  num_labels \u001b[39m=\u001b[39;49m num_of_labels)\n",
      "\u001b[1;32mc:\\Users\\The Doctor\\OneDrive - Georgia Institute of Technology\\Spring 2022\\Deep Learning\\Project\\Sesame-Street\\TaskPretraining_RoBERTa_Full.ipynb Cell 25'\u001b[0m in \u001b[0;36mfinetuning_loop\u001b[1;34m(num_models, training_args, dataset, adapter_name, num_labels, load_adapter, adapter_dir)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=7'>8</a>\u001b[0m writer \u001b[39m=\u001b[39m TensorBoardCallback(writer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=9'>10</a>\u001b[0m trainer \u001b[39m=\u001b[39m AdapterTrainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=10'>11</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=11'>12</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=15'>16</a>\u001b[0m     compute_metrics \u001b[39m=\u001b[39m compute_metric \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=16'>17</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=18'>19</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=20'>21</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtraining_args\u001b[39m.\u001b[39moutput_dir\u001b[39m}\u001b[39;00m\u001b[39m/evaulations.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/The%20Doctor/OneDrive%20-%20Georgia%20Institute%20of%20Technology/Spring%202022/Deep%20Learning/Project/Sesame-Street/TaskPretraining_RoBERTa_Full.ipynb#ch0000024?line=21'>22</a>\u001b[0m f\u001b[39m.\u001b[39mwrite(adapter)\n",
      "File \u001b[1;32m~\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\trainer.py:1377\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1373'>1374</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1374'>1375</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1376'>1377</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1377'>1378</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1378'>1379</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1379'>1380</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1380'>1381</a>\u001b[0m ):\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1381'>1382</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1382'>1383</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   <a href='file:///c%3A/Users/The%20Doctor/.conda/envs/pytorch/lib/site-packages/transformers/trainer.py?line=1383'>1384</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"TAPT_sci-erc\",\n",
    "                 load_adapter = True,\n",
    "                 adapter_dir = \"./training_output/pretraining/TAPT\",\n",
    "                 num_labels = num_of_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=100,\n",
    "    output_dir=\"./training_output/finetuning/No_Pretrain\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    # load_best_model_at_end = True,\n",
    "    save_steps = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning_loop(num_models = 5, \n",
    "                 training_args = training_args, \n",
    "                 dataset = scierc_dataset_finetuning,  \n",
    "                 adapter_name = \"sci-erc\",\n",
    "                 load_adapter = False,\n",
    "                 num_labels = num_of_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2469684d7fc26b35c5046a1b9c559332af356cccfef9c5dca4231e653832987c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
