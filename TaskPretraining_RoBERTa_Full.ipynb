{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qfcJzcz7xZsE"},"outputs":[],"source":["#Loading Libraries\n","# %conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n","# %pip install -U adapter-transformers\n","# %conda install -y -c conda-forge tensorboard\n","# %pip install optuna\n","# %pip install tqdm\n","# from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxq9aKCexZsK","outputId":"242a5fd8-f8a1-4c0f-94ed-87ae58295a18"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration nsusemiehl--SciERC-f57c64a52b9c80c0\n","Reusing dataset json (C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n","100%|██████████| 3/3 [00:00<00:00, 999.12it/s]"]},{"name":"stdout","output_type":"stream","text":["{'train': 3219, 'test': 974, 'validation': 455}\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Loading dataset\n","from datasets import load_dataset\n","\n","scierc_name = 'nsusemiehl/SciERC'\n","scierc_dataset = load_dataset(scierc_name)\n","print(scierc_dataset.num_rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Np7F8dfQxZsL","outputId":"e3721d9f-7620-412e-e5d4-329b6096d0b5"},"outputs":[{"data":{"text/plain":["{'text': 'We present two [[ methods ]] for capturing << nonstationary chaos >> , then present a few examples including biological signals , ocean waves and traffic flow .',\n"," 'label': 'USED-FOR',\n"," 'metadata': [3, 3, 6, 7]}"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["scierc_dataset['train'][255]"]},{"cell_type":"markdown","metadata":{"id":"bgjIsnc3xZsL"},"source":["This block creates dataset for pretraining"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wn5XEPGixZsL","outputId":"07283772-6700-4e4e-8a4e-3faec5efed4c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-74c87c445e04c867.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-876e0d37d233bb91.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-ecd8f07822814f57.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-b972bbb5f00d5d6e.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-40cee27cca22a664.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-2ba76227dbf9fc66.arrow\n"]}],"source":["from transformers import RobertaTokenizer\n","\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","# Tokenize the set for the transformer\n","def encode_batch_pretraining(batch):\n","  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n","  return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n","\n","# Encode the input data\n","# NOTE: num_proc does not seem to work, for some reason it can't find the tokenizer\n","scierc_dataset_pretraining = scierc_dataset.map(encode_batch_pretraining, \n","                                    batched=True, \n","                                    remove_columns=scierc_dataset['train'].column_names, \n","                                    )\n","\n","# We make the labels the same as the input as this is language learning \n","def add_labels(examples):\n","  examples[\"labels\"] = examples[\"input_ids\"].copy()\n","  return examples\n","  \n","scierc_dataset_pretraining = scierc_dataset_pretraining.map(add_labels, batched=True)\n","scierc_dataset_pretraining.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tJ1NmQKxZsM"},"outputs":[],"source":["# Collater adds padding in the form of EOS tokens, makes data augmentations of random masking ('mlm_probability)\n","from transformers import DataCollatorForLanguageModeling\n","\n","tokenizer.pad_token = tokenizer.eos_token\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"]},{"cell_type":"markdown","metadata":{"id":"Wy5F-H13xZsM"},"source":["Here we are creating the dataset for task finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjtU6uxGxZsM","outputId":"82541c7b-ee19-400a-9353-78572564ebdf"},"outputs":[{"name":"stdout","output_type":"stream","text":["['COMPARE' 'CONJUNCTION' 'EVALUATE-FOR' 'FEATURE-OF' 'HYPONYM-OF'\n"," 'PART-OF' 'USED-FOR']\n","7\n"]}],"source":["# Finding the number of labels\n","import numpy as np\n","labels = np.unique(np.array(scierc_dataset['train']['label']))\n","num_of_labels = labels.size\n","\n","print(labels)\n","print(num_of_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sauqKW9pxZsM","outputId":"c405a6d3-9bfe-4ba3-8476-aebf44451b01"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-3fa4decd4606a523.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-4d2e52dbe4cdbad6.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-214c6724dd02783d.arrow\n"]},{"data":{"text/plain":["{'text': 'The agreement in question involves number in [[ nouns ]] and << reflexive pronouns >> and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .',\n"," 'label': 1,\n"," 'metadata': [7, 7, 9, 10]}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# encoding the labels\n","def encode_labels(dataset):\n","    for i in range(num_of_labels):\n","        if dataset['label'] == labels[i]:\n","            dataset['label'] = i\n","    return dataset\n","\n","scierc_dataset = scierc_dataset.map(encode_labels)\n","scierc_dataset['train'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKHGX7p1xZsM","outputId":"89c9f1d6-a8ac-4107-c730-b6ace151d82f"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-6c719cda162c2a70.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-a2e89e74a8a70442.arrow\n","Loading cached processed dataset at C:\\Users\\The Doctor\\.cache\\huggingface\\datasets\\json\\nsusemiehl--SciERC-f57c64a52b9c80c0\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b\\cache-1e077601566683c7.arrow\n"]}],"source":["def encode_batch_finetuning(batch):\n","  \"\"\"Encodes a batch of input data using the model tokenizer.\"\"\"\n","  return tokenizer(batch[\"text\"], max_length=128, truncation=True, padding=\"max_length\")\n","\n","# Encode the input data\n","scierc_dataset_finetuning = scierc_dataset.map(encode_batch_finetuning, batched=True)\n","# The transformers model expects the target class column to be named \"labels\"\n","scierc_dataset_finetuning = scierc_dataset_finetuning.rename_column(\"label\", 'labels')\n","# Transform to pytorch tensors and only output the required columns\n","scierc_dataset_finetuning.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"]},{"cell_type":"markdown","metadata":{"id":"Aeu0hivHxZsM"},"source":["# Model Creation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12XyluNgxZsM"},"outputs":[],"source":["from transformers import RobertaConfig\n","from transformers import RobertaAdapterModel\n","\n","def model_init(adapter_name = 'default_adapter', \n","               num_lables = 0, \n","               pretraining = False,\n","               load_adapter = False,\n","               adapter_dir = 'path'):\n","    \"\"\"Creates a new roBERTa model with the given name for its adapter.\n","\n","    Args:\n","        adapter_name (str): The name of the adapter to load/create. Defaults to 'default_adapter'.\n","        num_lables (int, optional): The number of labels for classification task. Defaults to 0.\n","        pretraining (bool, optional): Whether to create a model for pretraining or classification. Defaults to False.\n","        load_adapter (bool, optional): Whether to load an adapter with the adapter_name given or create a new one. Defaults to False.\n","        adapter_dir (str, optional): Directory to load the adapter. If load_adapter you need to specify this.  Defaults to 'path'.\n","\n","    Returns:\n","        RobertaAdapterModel: A roBERTA model with an adapter added to it.\n","    \"\"\"\n","    \n","    if pretraining:\n","        config = RobertaConfig.from_pretrained(\n","            \"roberta-base\",\n","            # num_labels=num_of_labels,*-8536.22.03\n","        )\n","        model = RobertaAdapterModel.from_pretrained(\n","            \"roberta-base\",\n","            config=config,\n","        )\n","        if load_adapter:\n","            # Add new adapter\n","            model.load_adapter(adapter_dir)\n","\n","        else:\n","            # Add new adapter\n","            model.add_adapter(adapter_name)\n","            \n","        # Add a matching classification head\n","        model.add_masked_lm_head(adapter_name)\n","            \n","    else:\n","        config = RobertaConfig.from_pretrained(\n","            \"roberta-base\",\n","            num_labels=num_lables,\n","        )\n","        model = RobertaAdapterModel.from_pretrained(\n","            \"roberta-base\",\n","            config=config,\n","        )\n","        \n","        if load_adapter:\n","            # Add new adapter\n","            model.load_adapter(adapter_dir)\n","\n","        else:\n","            # Add new adapter\n","            model.add_adapter(adapter_name)\n","            \n","        # Add a matching classification head\n","        model.add_classification_head(\n","                adapter_name,\n","                num_labels=num_lables,\n","                id2label={0:'COMPARE', 1:'CONJUNCTION', 2:'EVALUATE-FOR', \n","                        3:'FEATURE-OF', 4:'HYPONYM-OF', 5:'PART-OF', 6:'USED-FOR'},\n","                overwrite_ok = True)\n","            \n","    # Activate the adapter\n","    model.train_adapter(adapter_name)    \n","     \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"5z5e8YKfxZsN"},"source":["Pretraining Block"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5cm9YzvxZsN"},"outputs":[],"source":["from transformers import TrainingArguments, AdapterTrainer\n","from datasets import load_metric\n","from torch.utils.tensorboard import SummaryWriter\n","from transformers.integrations import TensorBoardCallback\n","\n","import json\n","\n","def pretraining_loop(num_models, training_args, dataset, \n","                     data_collator, adapter_name, \n","                    #  DAPT_n_TAPT, TAPT_dataset\n","                     ):\n","    \"\"\"The Loop for running num_models number of models to account for run2run variance. Will run the model \n","        and evaluate.\n","\n","    Args:\n","        num_models (int): Number of models to loop through\n","        training_args (transformers.TrainingArguments): The arguments to pass to the trainer\n","        dataset (dataset): The dataset to train on\n","        data_collator (data_collator): The data collator for the trainer to use\n","        adapter_name (str): Name of the adapter to create\n","    \"\"\"\n","\n","    for i in range(num_models):\n","        adapter = f\"{adapter_name}_{i}\"\n","        model = model_init(adapter_name = adapter, pretraining=True)\n","        \n","        writer = SummaryWriter(log_dir= f'runs/{adapter}')\n","        writer = TensorBoardCallback(writer)\n","\n","        trainer = AdapterTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=dataset[\"train\"],\n","            eval_dataset=dataset[\"validation\"],\n","            data_collator=data_collator,  \n","            callbacks=[writer] \n","        )\n","        \n","        trainer.train()\n","        \n","        f = open(f\"{training_args.output_dir}/evaulations.txt\", \"a\")\n","        f.write(adapter)\n","        f.write(json.dumps(trainer.evaluate(dataset['test'])))\n","        f.write('\\n')\n","        f.close()\n","        \n","        model.save_all_adapters(training_args.output_dir, with_head=False)\n","        # model.save_pretrained(f\"{adapter_name}\")\n","        \n","        # if DAPT_n_TAPT:\n","        #     trainer = AdapterTrainer(\n","        #         model=model,\n","        #         args=training_args,\n","        #         train_dataset=TAPT_dataset[\"train\"],\n","        #         eval_dataset=TAPT_dataset[\"validation\"],\n","        #         data_collator=data_collator,  \n","        #         callbacks=[writer] \n","        #     )\n","            \n","        #     trainer.train()\n","        \n","        #     f = open(\"DAPT_TAPT_evaulations.txt\", \"a\")\n","        #     f.write(adapter_name)\n","        #     f.write(trainer.evaluate(TAPT_dataset['test']))\n","        #     f.write('\\n')\n","        #     f.close()\n","            \n","        #     model.save_pretrained(f\"{adapter_name}_DAPT_TAPT\")"]},{"cell_type":"markdown","metadata":{"id":"BHa7jBwexZsN"},"source":["DAPT Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4-2DrGOxZsN"},"outputs":[],"source":["training_args = TrainingArguments(\n","    learning_rate=5e-4,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    logging_steps=100,\n","    output_dir=\"./training_output/pretraining/DAPT\",\n","    overwrite_output_dir=True,\n","    # The next line is important to ensure the dataset labels are properly passed to the model\n","    remove_unused_columns=True,\n","    evaluation_strategy = 'steps',\n","    # load_best_model_at_end = True,\n","    save_steps = 100,\n","    gradient_accumulation_steps = 64,\n","    warmup_ratio = 0.06,\n","    weight_decay=0.01,\n","    adam_epsilon = 1e-6,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p8fW_E_WxZsN"},"outputs":[],"source":["pretraining_loop(num_models = 5, \n","                 training_args = training_args, \n","                #  dataset = DAPT_dataset, TODO: Need to add DAPT training set\n","                 data_collator = data_collator, \n","                 adapter_name = \"DAPT_sci-erc\")"]},{"cell_type":"markdown","metadata":{"id":"VlCBw7Z5xZsN"},"source":["DAPT+TAPT Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m12lLWeNxZsN"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"XdKpZRW0xZsO"},"source":["TAPT Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OP6QeroyxZsO"},"outputs":[],"source":["training_args = TrainingArguments(\n","    learning_rate=0.0001,\n","    num_train_epochs=100,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    logging_steps=10,\n","    output_dir=\"./training_output/pretraining/TAPT\",\n","    overwrite_output_dir=True,\n","    # The next line is important to ensure the dataset labels are properly passed to the model\n","    remove_unused_columns=True,\n","    evaluation_strategy = 'steps',\n","    # load_best_model_at_end = True,\n","    save_steps = 100,\n","    gradient_accumulation_steps = 8,\n","    warmup_ratio = 0.06,\n","    # load_best_model_at_end = True,\n","    weight_decay=0.01,\n","    adam_epsilon = 1e-6,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eLVlFixnxZsO"},"outputs":[],"source":["pretraining_loop(num_models = 1, \n","                 training_args = training_args, \n","                 dataset = scierc_dataset_pretraining, \n","                 data_collator = data_collator, \n","                 adapter_name = \"TAPT_sci-erc\")"]},{"cell_type":"markdown","metadata":{"id":"IItDd1jdxZsO"},"source":["Fine Tuning Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"euscik0hxZsO"},"outputs":[],"source":["from datasets import load_metric\n","metric = load_metric('f1')\n","\n","def compute_metric(EvalPrediction):\n","  \n","  logits, labels = EvalPrediction\n","  predictions = np.argmax(logits, axis=-1)\n","  return metric.compute(predictions=predictions, references=labels, average= 'macro')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOQ3pSH3xZsO"},"outputs":[],"source":["def finetuning_loop(num_models, training_args, dataset, adapter_name, num_labels, load_adapter = False, adapter_dir = 'Path'):\n","    \"\"\"The loop for finetuning num_models number of models to account for run2run variance\n","\n","    Args:\n","        num_models (int): Number of models to loop through\n","        training_args (transformers.TrainingArguments): The arguments to pass to the trainer\n","        dataset (dataset): The dataset to train on\n","        adapter_name (str): Name of the adapter to create/load\n","        num_labels (int): Number of labels for classification task\n","        load_adapter (bool, optional): Whether to load the adapter based on adapter_name. Defaults to False.\n","        adapter_dir (str, optional): Path to the adapter to load when load_adapter is True. Defaults to 'Path'.\n","    \"\"\"\n","\n","    for i in range(num_models):\n","        adapter = f\"{adapter_name}_{i}\"\n","        model = model_init(adapter_name = adapter, num_lables = num_labels, pretraining=False, load_adapter = load_adapter, adapter_dir = f\"{adapter_dir}/{adapter}\")\n","        \n","        writer = SummaryWriter(log_dir= f'runs/{adapter}')\n","        writer = TensorBoardCallback(writer)\n","\n","        trainer = AdapterTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=dataset[\"train\"],\n","            eval_dataset=dataset[\"validation\"],\n","            callbacks=[writer],\n","            compute_metrics = compute_metric \n","        )\n","        \n","        trainer.train()\n","        \n","        f = open(f\"{training_args.output_dir}/evaulations.txt\", \"a\")\n","        f.write(adapter)\n","        f.write(json.dumps(trainer.evaluate(dataset['test'])))\n","        f.write('\\n')\n","        f.close()\n","        \n","        # model.save_pretrained(f\"{adapter_name}\")\n","        model.save_all_adapters(training_args.output_dir)\n","        \n","        trainer.remove_callback(writer)"]},{"cell_type":"markdown","metadata":{"id":"oEd8VPfDxZsO"},"source":["DAPT Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0LKtYcgxZsO"},"outputs":[],"source":["training_args = TrainingArguments(\n","    learning_rate=1e-4,\n","    num_train_epochs=50,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    logging_steps=100,\n","    output_dir=\"./training_output/finetuning/DAPT\",\n","    overwrite_output_dir=True,\n","    # The next line is important to ensure the dataset labels are properly passed to the model\n","    remove_unused_columns=False,\n","    evaluation_strategy = 'epoch',\n","    # load_best_model_at_end = True,\n","    save_steps = 100\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"peXjRCd_xZsP"},"outputs":[],"source":["finetuning_loop(num_models = 5, \n","                training_args = training_args, \n","                dataset = scierc_dataset_finetuning,  \n","                adapter_name = \"DAPT_sci-erc\",\n","                load_adapter = True)"]},{"cell_type":"markdown","metadata":{"id":"XTPzaehUxZsP"},"source":["DAPT+TAPT Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BW6PKymOxZsP"},"outputs":[],"source":["training_args = TrainingArguments(\n","    learning_rate=1e-4,\n","    num_train_epochs=50,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    logging_steps=10,\n","    output_dir=\"./training_output/finetuning/DAPT_TAPT\",\n","    overwrite_output_dir=True,\n","    # The next line is important to ensure the dataset labels are properly passed to the model\n","    remove_unused_columns=False,\n","    evaluation_strategy = 'epoch',\n","    # load_best_model_at_end = True,\n","    save_steps = 100\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIr32zm6xZsP"},"outputs":[],"source":["finetuning_loop(num_models = 5, \n","                 training_args = training_args, \n","                 dataset = scierc_dataset_finetuning,  \n","                 adapter_name = \"DAPT_TAPT_sci-erc\",\n","                 load_adapter = True,\n","                 adapter_dir = \"./training_output/pretraining/DAPT_TAPT\",\n","                 num_labels = num_of_labels)"]},{"cell_type":"markdown","metadata":{"id":"8aUBEuafxZsP"},"source":["TAPT Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bqj8r-QxZsP","outputId":"3a12328d-8651-4503-9eb4-04287c1ac776"},"outputs":[{"name":"stderr","output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["# training_args = TrainingArguments(\n","#     learning_rate=2e-5,\n","#     num_train_epochs=50,\n","#     per_device_train_batch_size=16,\n","#     per_device_eval_batch_size=16,\n","#     logging_steps=100,\n","#     output_dir=\"./training_output/finetuning/TAPT\",\n","#     overwrite_output_dir=True,\n","#     # The next line is important to ensure the dataset labels are properly passed to the model\n","#     remove_unused_columns=False,\n","#     evaluation_strategy = 'epoch',\n","#     # load_best_model_at_end = True,\n","#     save_steps = 100,\n","#     lr_scheduler_type = 'constant',\n","#     log_level  = 'error'\n","    \n","# )\n","\n","training_args = TrainingArguments(\n","    learning_rate=1e-4,\n","    num_train_epochs=50,\n","    per_device_train_batch_size=128,\n","    per_device_eval_batch_size=128,\n","    logging_steps=100,\n","    output_dir=\"./training_output/finetuning/TAPT\",\n","    overwrite_output_dir=True,\n","    # The next line is important to ensure the dataset labels are properly passed to the model\n","    remove_unused_columns=False,\n","    evaluation_strategy = 'epoch',\n","    # load_best_model_at_end = True,\n","    save_steps = 100,\n","    # lr_scheduler_type = 'constant',\n","    log_level  = 'error'\n","    \n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0waNSR0xZsP"},"outputs":[],"source":["finetuning_loop(num_models = 1, \n","                 training_args = training_args, \n","                 dataset = scierc_dataset_finetuning,  \n","                 adapter_name = \"TAPT_sci-erc\",\n","                 load_adapter = True,\n","                 adapter_dir = \"./training_output/pretraining/TAPT\",\n","                 num_labels = num_of_labels)"]},{"cell_type":"markdown","metadata":{"id":"R6_GmpjsxZsQ"},"source":["Only Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFMWDT6oxZsQ","outputId":"325bc34a-ed58-4bfd-80e7-ff297d04c577"},"outputs":[{"name":"stderr","output_type":"stream","text":["using `logging_steps` to initialize `eval_steps` to 100\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["training_args = TrainingArguments(\n","    learning_rate=1e-4,\n","    num_train_epochs=50,\n","    per_device_train_batch_size=128,\n","    per_device_eval_batch_size=128,\n","    logging_steps=100,\n","    output_dir=\"./training_output/finetuning/No_Pretrain\",\n","    overwrite_output_dir=True,\n","    # The next line is important to ensure the dataset labels are properly passed to the model\n","    remove_unused_columns=True,\n","    evaluation_strategy = 'steps',\n","    # load_best_model_at_end = True,\n","    save_steps = 100,\n","    # lr_scheduler_type = 'constant',\n","    log_level  = 'error',\n","    disable_tqdm = False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5trArsIxZsQ","outputId":"a2adb729-1a3c-483b-913e-0047e50230e4"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\The Doctor/.cache\\huggingface\\transformers\\51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaAdapterModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaAdapterModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Adding adapter 'sci-erc_0'.\n","Adding head 'sci-erc_0' with config {'head_type': 'classification', 'num_labels': 7, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'COMPARE': 0, 'CONJUNCTION': 1, 'EVALUATE-FOR': 2, 'FEATURE-OF': 3, 'HYPONYM-OF': 4, 'PART-OF': 5, 'USED-FOR': 6}, 'use_pooler': False, 'bias': True}.\n","C:\\Users\\The Doctor\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","  8%|▊         | 100/1300 [01:02<12:47,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.4659, 'learning_rate': 9.230769230769232e-05, 'epoch': 3.85}\n"]},{"name":"stderr","output_type":"stream","text":["                                                  \n","  8%|▊         | 100/1300 [01:03<12:47,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.3007993698120117, 'eval_f1': 0.2376424095813536, 'eval_runtime': 1.2443, 'eval_samples_per_second': 365.68, 'eval_steps_per_second': 3.215, 'epoch': 3.85}\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 200/1300 [02:05<11:46,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.9639, 'learning_rate': 8.461538461538461e-05, 'epoch': 7.69}\n"]},{"name":"stderr","output_type":"stream","text":["                                                  \n"," 15%|█▌        | 200/1300 [02:07<11:46,  1.56it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.7327762842178345, 'eval_f1': 0.623693521343898, 'eval_runtime': 1.2594, 'eval_samples_per_second': 361.289, 'eval_steps_per_second': 3.176, 'epoch': 7.69}\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 300/1300 [03:09<10:47,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6248, 'learning_rate': 7.692307692307693e-05, 'epoch': 11.54}\n"]},{"name":"stderr","output_type":"stream","text":["                                                  \n"," 23%|██▎       | 300/1300 [03:10<10:47,  1.55it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.5487247705459595, 'eval_f1': 0.6924237521239599, 'eval_runtime': 1.2432, 'eval_samples_per_second': 366.003, 'eval_steps_per_second': 3.218, 'epoch': 11.54}\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███       | 400/1300 [04:12<09:10,  1.64it/s]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.4717, 'learning_rate': 6.923076923076924e-05, 'epoch': 15.38}\n"]},{"name":"stderr","output_type":"stream","text":["                                                  \n"," 31%|███       | 400/1300 [04:13<09:10,  1.64it/s]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 0.47530850768089294, 'eval_f1': 0.7786714706136352, 'eval_runtime': 1.2061, 'eval_samples_per_second': 377.25, 'eval_steps_per_second': 3.316, 'epoch': 15.38}\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 430/1300 [04:32<09:18,  1.56it/s]"]}],"source":["finetuning_loop(num_models = 1, \n","                 training_args = training_args, \n","                 dataset = scierc_dataset_finetuning,  \n","                 adapter_name = \"sci-erc\",\n","                 load_adapter = False,\n","                 num_labels = num_of_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4415VntxZsQ"},"outputs":[],"source":[""]}],"metadata":{"interpreter":{"hash":"2469684d7fc26b35c5046a1b9c559332af356cccfef9c5dca4231e653832987c"},"kernelspec":{"display_name":"Python 3.9.12 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4,"colab":{"name":"TaskPretraining_RoBERTa_Full.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}